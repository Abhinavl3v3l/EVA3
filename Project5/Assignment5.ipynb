{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhinavl3v3l/EVA3/blob/master/Assignment5_final_TBS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNyZv-Ec52ot",
        "colab_type": "text"
      },
      "source": [
        "# **Import Libraries and modules**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m3w1Cw49Zkt",
        "colab_type": "code",
        "outputId": "7acf62c7-d943-46aa-fffd-1fa1b91ac5b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eso6UHE080D4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add, BatchNormalization\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from keras.regularizers import l2\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras.datasets import mnist                                                # Importing Necessary Libraries and dataset(in keras)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zByEi95J86RD",
        "colab_type": "text"
      },
      "source": [
        "### Load pre-shuffled MNIST data into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eRM0QWN83PV",
        "colab_type": "code",
        "outputId": "5bed519d-0c98-4a69-976a-3ebe49961f5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()                        # mnist data divided into Training and Test Data\n",
        "                                                                                # Where X_train are 60K 28x28 gray scale training images of numbers from [0-9] \n",
        "                                                                                # y_train are label of numbers from [0-9]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a4Be72j8-ZC",
        "colab_type": "code",
        "outputId": "4d5a7338-7602-455a-8630-5b1e315867a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[0])                                                           # Displaying what one of the images look like."
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fd1b5d999e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSRB2IAnCDiTx/65XcTNOWsh5AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkmprriw9AnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)                           # (60000, 28, 28) ->  (60000, 28, 28, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09Fk2UgeuN3Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Image Normalization \n",
        "\n",
        "datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
        "datagen.fit(X_train)\n",
        "iterator = datagen.flow(X_train, y_train, batch_size=len(X_train), shuffle=False)\n",
        "X_train, y_train = iterator.next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2m4YS4E9CRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')                                             # Convert to float type\n",
        "X_test = X_test.astype('float32') \n",
        "X_train /= 255\n",
        "X_test /= 255                                                                   # Floating Value ranges from [0,1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mn0vAYD9DvB",
        "colab_type": "code",
        "outputId": "1190b32e-57bc-4d8f-9a00-b2ea057c0bfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y_train[:10]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG8JiXR39FHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, 10)                                  \n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYlFRvKS9HMB",
        "colab_type": "code",
        "outputId": "123bf22c-a8e9-4b8f-ef62-183dadae9299",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "Y_train[:10]                                                                    ## y_train to Y_train  - example,5 value is converted to an array where 5th element of array is 1(lit) and rest all is 0 \n",
        "                                                                                \n",
        "                                                                                # OR \n",
        "                                                                                \n",
        "                                                                                # 1 Hot Encoded  \n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCY5SFWYBoew",
        "colab_type": "text"
      },
      "source": [
        "### Assignment 5\n",
        "\n",
        "1. Change the code 8 or your own 4th Code from Assignment 4 to include:\n",
        "  - image normalization\n",
        "  - L2 regularization\n",
        "  - ReLU after BN\n",
        "2. Run your new code for 40 epochs and save the model with highest validation accuracy\n",
        "3. Find out 25 misclassified images from the validation dataset and create an image gallery\n",
        "4. Submit\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dzps5B8SbBvh",
        "colab_type": "code",
        "outputId": "b2346b74-b5f0-4566-87b9-ab80092c0cf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# Network \n",
        "\n",
        "from keras.layers import Activation\n",
        "model = Sequential()\n",
        "\n",
        "# L2 work for well for very large number of kernels but we don't have large number of kernel here, might backfire.\n",
        " \n",
        "model.add(BatchNormalization())#BN before RELU\n",
        "model.add(Convolution2D(12,( 3, 3), activation='relu', input_shape=(28,28,1)))    #26\n",
        "model.add(Dropout(0.1))\n",
        "len(model.layers)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpqoKoHQey2r",
        "colab_type": "code",
        "outputId": "90b48b27-941e-4138-d9e7-c140a00d93ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "w = model.layers[1].get_weights()\n",
        "w"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O00KBK7xaAIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "model.add(BatchNormalization())#BN before RELU\n",
        "\n",
        "model.add(Convolution2D(20, (3, 3), activation='relu'))                         #24\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "model.add(Convolution2D(10, (1, 1), activation='relu'))                           #24\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))                                       #12\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(16, (3, 3), activation='relu'))                           #10\n",
        "\n",
        "model.add(Dropout(0.1))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(16, (3, 3), activation='relu'))                           #8\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(16, (3, 3), activation='relu'))                           #6\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(16, (3, 3), activation='relu'))                           #4\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Convolution2D(10, 4))\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c4DAFg2kC4O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define custom loss\n",
        "def custom_loss(actual,predicted):\n",
        "    sqr_w = 0\n",
        "    lamda = 0.01 #1e-4\n",
        "\n",
        "    for layer in model.layers:\n",
        "       print(layer.get_weights())\n",
        "       sqr_w = sqr_w + np.sum(np.sum(np.sum(np.square(layer.get_weights()))))\n",
        "\n",
        "    l2_regularization = (lamda*sqr_w)/2*(bs)\n",
        "    loss = keras.losses.categorical_crossentropy(actual,predicted) + l2_regularization\n",
        "\n",
        "    # Return a function\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPv8w4lYkEfz",
        "colab_type": "code",
        "outputId": "58501b22-78ef-432b-896b-48228cd8ba53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "def scheduler(epoch, lr):\n",
        "  return round(0.003 * 1/(1 + 0.319 * epoch), 10)\n",
        "bs =128\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy',custom_loss])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ww4dlZ-kYOtS",
        "colab_type": "code",
        "outputId": "c07a2025-f64b-4ad2-bf4e-bd780c824110",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Running for 40 Epochs\n",
        "\n",
        "model.fit(X_train, Y_train, batch_size=bs, epochs=40, verbose=1, validation_data=(X_test, Y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "[array([1.], dtype=float32), array([0.], dtype=float32), array([0.], dtype=float32), array([1.], dtype=float32)]\n",
            "[array([[[[ 0.04039453,  0.12600012,  0.08220027, -0.14303634,\n",
            "          -0.14214814,  0.05413608, -0.1564173 , -0.015613  ,\n",
            "          -0.10957072,  0.01101641,  0.19804196,  0.20743956]],\n",
            "\n",
            "        [[-0.05648433, -0.11133818, -0.06272398,  0.17617466,\n",
            "           0.05326612,  0.11727203, -0.05391932, -0.13875586,\n",
            "          -0.03034569,  0.0954517 ,  0.12684233, -0.05589183]],\n",
            "\n",
            "        [[-0.09716354, -0.1841626 ,  0.0818048 , -0.04915017,\n",
            "           0.11086757, -0.01308315, -0.13172244, -0.15980291,\n",
            "          -0.04461378,  0.12966608,  0.15354712,  0.00535399]]],\n",
            "\n",
            "\n",
            "       [[[-0.02906631, -0.10854705, -0.18614867, -0.19500539,\n",
            "          -0.21141595,  0.16031642, -0.11269033,  0.00314234,\n",
            "           0.1581469 ,  0.13067247, -0.00148751,  0.18746029]],\n",
            "\n",
            "        [[-0.20984825, -0.03658436,  0.20377068, -0.05162182,\n",
            "           0.22228329,  0.20885305, -0.20271938,  0.12566803,\n",
            "           0.12386884,  0.16924398, -0.06405859,  0.1523631 ]],\n",
            "\n",
            "        [[-0.19678007,  0.08619966,  0.1935472 , -0.1813372 ,\n",
            "          -0.21708   , -0.10185106, -0.08348407, -0.21652773,\n",
            "           0.10837017,  0.00135902,  0.10983889, -0.04002814]]],\n",
            "\n",
            "\n",
            "       [[[-0.12773421,  0.21079965,  0.16595317,  0.03919317,\n",
            "           0.12349208, -0.07902531, -0.09473346, -0.13128668,\n",
            "          -0.1568346 , -0.1684083 , -0.18320027, -0.08579268]],\n",
            "\n",
            "        [[-0.03901117, -0.00133348,  0.10705526,  0.0909857 ,\n",
            "          -0.15943676,  0.21054189, -0.16511463, -0.13655555,\n",
            "           0.07135393,  0.09580584, -0.00787084, -0.14663452]],\n",
            "\n",
            "        [[-0.05099569,  0.06665649,  0.01356755, -0.12436668,\n",
            "           0.08207436,  0.02856208,  0.05962612, -0.10515171,\n",
            "          -0.19450337, -0.07545094,  0.13740073, -0.04571368]]]],\n",
            "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
            "[]\n",
            "[array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)]\n",
            "[array([[[[ 0.122035  , -0.04262874, -0.03511374, ...,  0.03543767,\n",
            "          -0.08839409, -0.09843924],\n",
            "         [-0.09112409,  0.13990316,  0.11097723, ...,  0.12210917,\n",
            "           0.04899667,  0.12911046],\n",
            "         [-0.1427635 ,  0.13910559,  0.03971721, ...,  0.01049331,\n",
            "          -0.04798137, -0.11959851],\n",
            "         ...,\n",
            "         [-0.07971399,  0.04392943, -0.02234507, ...,  0.1392028 ,\n",
            "           0.00378375,  0.12793678],\n",
            "         [ 0.0179394 ,  0.08323555, -0.00017089, ...,  0.04413368,\n",
            "           0.08024631, -0.03837005],\n",
            "         [-0.09627244,  0.00513683, -0.03084457, ..., -0.05309273,\n",
            "          -0.10014697,  0.08308753]],\n",
            "\n",
            "        [[ 0.1278913 ,  0.05316609,  0.11247212, ...,  0.12570038,\n",
            "           0.07408148,  0.14193806],\n",
            "         [ 0.07262871, -0.05144566,  0.0744414 , ...,  0.00854923,\n",
            "           0.11260322, -0.02059796],\n",
            "         [ 0.1075345 ,  0.04769814,  0.01914842, ..., -0.06344254,\n",
            "          -0.0505247 ,  0.05673228],\n",
            "         ...,\n",
            "         [-0.10901958, -0.11638856, -0.05434838, ..., -0.116592  ,\n",
            "           0.10131639,  0.10784665],\n",
            "         [-0.05830695,  0.12892753, -0.09089565, ..., -0.10426264,\n",
            "           0.13938871, -0.05407497],\n",
            "         [-0.10266376, -0.14402406, -0.05355864, ...,  0.14109191,\n",
            "           0.0454215 , -0.06080985]],\n",
            "\n",
            "        [[-0.12822132, -0.11285281,  0.1226975 , ...,  0.08495164,\n",
            "           0.10073134, -0.12842269],\n",
            "         [-0.0903776 ,  0.08771323,  0.07525423, ..., -0.10103235,\n",
            "           0.08873536, -0.07730919],\n",
            "         [ 0.1115455 ,  0.11036989, -0.11711311, ..., -0.09121036,\n",
            "           0.01281324, -0.00260787],\n",
            "         ...,\n",
            "         [ 0.10353556,  0.07968773, -0.13411684, ..., -0.1247512 ,\n",
            "           0.06684124,  0.0021138 ],\n",
            "         [ 0.11063471,  0.13104424, -0.03627855, ...,  0.1021661 ,\n",
            "           0.01100661,  0.11801481],\n",
            "         [-0.10078578,  0.10722968,  0.1170831 , ..., -0.05842942,\n",
            "          -0.0810587 ,  0.08900794]]],\n",
            "\n",
            "\n",
            "       [[[ 0.06064275, -0.09875707, -0.04759773, ..., -0.08313286,\n",
            "          -0.10593691,  0.04902008],\n",
            "         [ 0.05246107,  0.00141646,  0.11696443, ...,  0.14419612,\n",
            "           0.00830321, -0.0574426 ],\n",
            "         [ 0.02028051,  0.11072153,  0.13274455, ..., -0.12132276,\n",
            "           0.1194264 ,  0.13908154],\n",
            "         ...,\n",
            "         [-0.02699306,  0.14203092, -0.01968971, ..., -0.1038204 ,\n",
            "          -0.14097884,  0.04512015],\n",
            "         [-0.1428645 , -0.0262168 , -0.06816221, ..., -0.00995249,\n",
            "          -0.11018297, -0.03947563],\n",
            "         [-0.11196493, -0.12950127, -0.0312036 , ..., -0.11088675,\n",
            "          -0.11757235,  0.13423884]],\n",
            "\n",
            "        [[-0.09800144,  0.03748089, -0.01669019, ...,  0.06026162,\n",
            "          -0.1309798 ,  0.14161307],\n",
            "         [ 0.11691666, -0.08577913, -0.1308063 , ...,  0.04743162,\n",
            "          -0.06179705, -0.11192618],\n",
            "         [-0.12526715,  0.11364666,  0.00212915, ...,  0.14126694,\n",
            "          -0.10586406,  0.12080392],\n",
            "         ...,\n",
            "         [ 0.06548414, -0.14375131,  0.06266274, ...,  0.0480696 ,\n",
            "          -0.12054503, -0.08297318],\n",
            "         [ 0.09705825,  0.0082697 ,  0.02863169, ...,  0.07670875,\n",
            "           0.08393729,  0.11771968],\n",
            "         [ 0.14032575, -0.09686461, -0.13655336, ..., -0.00509825,\n",
            "           0.04661968, -0.04976872]],\n",
            "\n",
            "        [[-0.05748627, -0.04059924,  0.02229974, ..., -0.13040464,\n",
            "           0.05073851,  0.00991091],\n",
            "         [ 0.04496026, -0.04278111, -0.02070158, ...,  0.03957285,\n",
            "           0.04679736,  0.11823291],\n",
            "         [ 0.11137676, -0.10516091, -0.0580418 , ...,  0.09091444,\n",
            "          -0.04100359,  0.02909505],\n",
            "         ...,\n",
            "         [ 0.06455764, -0.07407105,  0.09350862, ...,  0.04266603,\n",
            "           0.00576638,  0.09371093],\n",
            "         [-0.0356598 ,  0.11059844, -0.11339448, ...,  0.10552868,\n",
            "          -0.06475524, -0.01957528],\n",
            "         [ 0.11561531, -0.0959281 , -0.0016759 , ...,  0.09657669,\n",
            "           0.03018105,  0.1198447 ]]],\n",
            "\n",
            "\n",
            "       [[[-0.10913335,  0.0707963 ,  0.10378674, ...,  0.05575927,\n",
            "           0.0137915 ,  0.06008905],\n",
            "         [ 0.08080621, -0.00675133, -0.07706851, ..., -0.09580016,\n",
            "           0.03985482, -0.01133236],\n",
            "         [ 0.12898648, -0.07519945, -0.12111257, ..., -0.07158284,\n",
            "           0.06801775, -0.05826947],\n",
            "         ...,\n",
            "         [ 0.0102773 ,  0.03601292, -0.02361641, ...,  0.05303492,\n",
            "          -0.04164257, -0.03954577],\n",
            "         [-0.106559  ,  0.07637867, -0.06370579, ...,  0.09510124,\n",
            "          -0.11586534,  0.00081554],\n",
            "         [ 0.01344459, -0.02621312,  0.05082984, ..., -0.06806647,\n",
            "           0.04935625,  0.00951175]],\n",
            "\n",
            "        [[-0.11472937,  0.07205574,  0.05617711, ...,  0.12566936,\n",
            "          -0.11189872,  0.08388551],\n",
            "         [-0.08131105, -0.13438863,  0.05756783, ...,  0.00752184,\n",
            "           0.13460377, -0.12947132],\n",
            "         [-0.11286789,  0.05543309,  0.1092273 , ..., -0.13372752,\n",
            "           0.11300033, -0.07745606],\n",
            "         ...,\n",
            "         [ 0.09339581, -0.02817892, -0.08658116, ..., -0.01337655,\n",
            "          -0.04324169, -0.02859318],\n",
            "         [ 0.04345234, -0.11321628, -0.05272461, ...,  0.01984277,\n",
            "           0.08718029,  0.00065495],\n",
            "         [-0.06102139,  0.04087588, -0.08328758, ...,  0.10906824,\n",
            "          -0.04706564, -0.11027238]],\n",
            "\n",
            "        [[-0.09583516,  0.10031676, -0.13371947, ...,  0.03328192,\n",
            "           0.02534555,  0.12156293],\n",
            "         [ 0.03821258, -0.0627785 , -0.07745448, ...,  0.14342621,\n",
            "          -0.07575618,  0.09030803],\n",
            "         [ 0.1424292 , -0.04177333,  0.03771022, ..., -0.12920432,\n",
            "           0.09352107,  0.13411883],\n",
            "         ...,\n",
            "         [-0.08273897, -0.10148652, -0.01250997, ..., -0.03417758,\n",
            "          -0.03427751, -0.04232408],\n",
            "         [ 0.09488988, -0.09904143, -0.12931575, ..., -0.12889914,\n",
            "           0.08203736,  0.01397647],\n",
            "         [-0.1156853 , -0.10422726,  0.07507704, ..., -0.04422755,\n",
            "           0.06407882, -0.00302839]]]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0.], dtype=float32)]\n",
            "[]\n",
            "[array([[[[ 0.06617004,  0.26334858,  0.04763737,  0.44113594,\n",
            "           0.17591846,  0.37199342, -0.12270868,  0.42382592,\n",
            "          -0.28578502, -0.41530123],\n",
            "         [ 0.21262014, -0.23179232, -0.0857996 ,  0.38715845,\n",
            "          -0.18546301,  0.35889006, -0.24768709, -0.29423264,\n",
            "          -0.00521934, -0.07243258],\n",
            "         [ 0.2139985 ,  0.23831218, -0.12009394, -0.43425065,\n",
            "           0.12976485, -0.22050159,  0.235345  ,  0.36205727,\n",
            "          -0.14001739,  0.08393294],\n",
            "         [-0.33480862, -0.43575075,  0.1748535 ,  0.24328959,\n",
            "           0.03805432,  0.06892091,  0.27887523, -0.20122503,\n",
            "           0.13102806,  0.15346867],\n",
            "         [ 0.17053765,  0.1942792 ,  0.02910259, -0.08175546,\n",
            "          -0.18747991, -0.21104169,  0.25936574, -0.21096002,\n",
            "          -0.22495058, -0.0893473 ],\n",
            "         [-0.08961484,  0.04802805,  0.1192916 ,  0.40765488,\n",
            "          -0.0671193 ,  0.37704974, -0.43100876, -0.06808072,\n",
            "           0.34947485, -0.07248726],\n",
            "         [ 0.13501108,  0.25844032,  0.12122756, -0.07397011,\n",
            "          -0.07769948,  0.4063216 , -0.41500023,  0.02740708,\n",
            "          -0.08277309,  0.30628562],\n",
            "         [-0.40793115, -0.1082398 ,  0.1828534 ,  0.40202355,\n",
            "           0.13275993, -0.4140611 ,  0.31609672,  0.07693309,\n",
            "          -0.17509031, -0.25336576],\n",
            "         [ 0.29960775,  0.32332873, -0.40834904,  0.19927794,\n",
            "          -0.07911131,  0.42713022, -0.40369648,  0.13428986,\n",
            "          -0.04393378,  0.3264022 ],\n",
            "         [-0.42064598,  0.19535452, -0.12033567,  0.30695564,\n",
            "          -0.109999  , -0.44582343,  0.10713232, -0.05324483,\n",
            "          -0.14780381, -0.10363898],\n",
            "         [ 0.15674192,  0.21357411,  0.30019033, -0.229957  ,\n",
            "          -0.24808991, -0.10224572, -0.26423568,  0.27082014,\n",
            "           0.10731018, -0.17876968],\n",
            "         [ 0.00071853,  0.38031298,  0.04441467,  0.24154115,\n",
            "          -0.1622374 ,  0.11294866,  0.03465557,  0.06108618,\n",
            "           0.29401267, -0.20243788],\n",
            "         [-0.41399488, -0.04811624, -0.15790197, -0.17042646,\n",
            "           0.04750016, -0.31257248,  0.10178137,  0.02990463,\n",
            "          -0.08405653,  0.4209084 ],\n",
            "         [-0.1942065 , -0.20437758, -0.39640734,  0.34698415,\n",
            "           0.20443738, -0.3564098 ,  0.06107491, -0.21076211,\n",
            "          -0.15951541, -0.04916072],\n",
            "         [ 0.08102667,  0.03918722,  0.43295836, -0.14412498,\n",
            "          -0.43396512, -0.3054082 ,  0.23757786,  0.2451585 ,\n",
            "           0.1179136 ,  0.19738454],\n",
            "         [ 0.4379545 ,  0.2642697 , -0.04129145, -0.3503607 ,\n",
            "          -0.3317355 ,  0.11796391,  0.2022351 ,  0.30841875,\n",
            "           0.38561028,  0.3386135 ],\n",
            "         [-0.11127871,  0.31768608, -0.36735198, -0.2142822 ,\n",
            "           0.00829619, -0.4135142 , -0.04461884,  0.3340096 ,\n",
            "          -0.35061893, -0.24712624],\n",
            "         [-0.36399993, -0.24667789, -0.27200663,  0.29055858,\n",
            "          -0.30297667,  0.2853706 , -0.26188534, -0.3194457 ,\n",
            "           0.10540402,  0.39926648],\n",
            "         [ 0.4077316 , -0.3508885 , -0.14340931,  0.14323968,\n",
            "          -0.31858927,  0.06568229,  0.4077742 ,  0.10789478,\n",
            "           0.05740339, -0.14951259],\n",
            "         [ 0.1205439 , -0.3602746 ,  0.26800495, -0.25461754,\n",
            "          -0.25461957, -0.410034  , -0.4251632 , -0.19214493,\n",
            "           0.15012652,  0.3999462 ]]]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
            "[]\n",
            "[array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)]\n",
            "[array([[[[-0.02606888, -0.01111428, -0.00207274, ...,  0.11850494,\n",
            "           0.03208794, -0.14175552],\n",
            "         [ 0.14966574, -0.1554211 , -0.09566887, ...,  0.05492952,\n",
            "          -0.03327008, -0.04021399],\n",
            "         [-0.13890839, -0.13540068, -0.07392883, ...,  0.14110631,\n",
            "          -0.06542929,  0.00532909],\n",
            "         ...,\n",
            "         [-0.12391891,  0.11756241,  0.04396868, ..., -0.06720798,\n",
            "          -0.0159981 , -0.01429443],\n",
            "         [-0.05567989, -0.10600433, -0.05333671, ...,  0.15923944,\n",
            "          -0.14526246, -0.11013781],\n",
            "         [ 0.14081106, -0.00966002, -0.01992913, ..., -0.14862613,\n",
            "          -0.10439251,  0.0291556 ]],\n",
            "\n",
            "        [[ 0.13377008,  0.13728207, -0.10114914, ...,  0.11780035,\n",
            "          -0.05788419, -0.08857285],\n",
            "         [-0.08847252,  0.03009656,  0.15732974, ...,  0.04532868,\n",
            "          -0.15149926,  0.02453353],\n",
            "         [ 0.13342106,  0.08746368, -0.09115888, ..., -0.1262751 ,\n",
            "           0.05775611,  0.07610613],\n",
            "         ...,\n",
            "         [ 0.03635623, -0.07996046,  0.06939025, ...,  0.1314466 ,\n",
            "          -0.06660084,  0.1099765 ],\n",
            "         [ 0.06895113, -0.10222028, -0.03797213, ...,  0.14647421,\n",
            "          -0.04058824, -0.0846054 ],\n",
            "         [ 0.00053914,  0.12177855,  0.04116651, ...,  0.12014705,\n",
            "           0.10226408, -0.11472563]],\n",
            "\n",
            "        [[ 0.14629784, -0.07388374, -0.15135297, ..., -0.13632232,\n",
            "          -0.10883072,  0.08229929],\n",
            "         [ 0.0851403 , -0.12288032, -0.08355285, ...,  0.1366907 ,\n",
            "           0.13638589,  0.03548071],\n",
            "         [ 0.04164875, -0.04153833,  0.00219814, ..., -0.01170062,\n",
            "          -0.08075982,  0.07410441],\n",
            "         ...,\n",
            "         [-0.08070267, -0.00764135, -0.01681155, ...,  0.09456837,\n",
            "          -0.04600064,  0.05251308],\n",
            "         [ 0.03274055, -0.14557342, -0.09441371, ..., -0.08321013,\n",
            "          -0.09564669,  0.07694875],\n",
            "         [ 0.15748411, -0.15610003,  0.00939927, ..., -0.05488703,\n",
            "           0.10837856, -0.08666153]]],\n",
            "\n",
            "\n",
            "       [[[-0.10666755, -0.10439018,  0.09156421, ...,  0.15650272,\n",
            "          -0.03192909, -0.11042158],\n",
            "         [ 0.09398514, -0.01963867, -0.00891228, ..., -0.01535118,\n",
            "           0.03343156, -0.08756535],\n",
            "         [-0.00227916,  0.14611986, -0.08083877, ...,  0.1320315 ,\n",
            "          -0.07319074,  0.09696653],\n",
            "         ...,\n",
            "         [ 0.15197691,  0.11789125, -0.04176564, ...,  0.12454364,\n",
            "          -0.01710765, -0.13902502],\n",
            "         [-0.0406604 , -0.113514  ,  0.13242519, ...,  0.08839189,\n",
            "          -0.07200155,  0.05201627],\n",
            "         [-0.02100493,  0.01111194,  0.03656727, ...,  0.04904288,\n",
            "           0.00494556, -0.10413317]],\n",
            "\n",
            "        [[-0.04681759,  0.13730139, -0.11231694, ..., -0.13305372,\n",
            "          -0.01720588,  0.1054855 ],\n",
            "         [ 0.01600882,  0.0181265 , -0.03585187, ..., -0.14236307,\n",
            "           0.09474784, -0.01707834],\n",
            "         [ 0.05957396,  0.08970661, -0.14132605, ..., -0.16009794,\n",
            "           0.05049497, -0.04534711],\n",
            "         ...,\n",
            "         [-0.00282475,  0.0539474 , -0.1314787 , ...,  0.02297214,\n",
            "          -0.13172857, -0.14498316],\n",
            "         [-0.1524532 ,  0.14608479,  0.09292674, ..., -0.08283152,\n",
            "           0.01554306, -0.01997635],\n",
            "         [ 0.10593572, -0.10545263,  0.08962797, ..., -0.0560025 ,\n",
            "           0.13283756, -0.09466587]],\n",
            "\n",
            "        [[ 0.03587218,  0.03955719,  0.1393995 , ..., -0.01252754,\n",
            "          -0.1322925 , -0.05728874],\n",
            "         [-0.01216149, -0.04982335, -0.08495545, ...,  0.05094187,\n",
            "          -0.09011339,  0.01524635],\n",
            "         [-0.04106382, -0.04288276,  0.04237537, ..., -0.05069529,\n",
            "          -0.14242305, -0.03121494],\n",
            "         ...,\n",
            "         [ 0.00158654, -0.05308452,  0.06110179, ...,  0.13132542,\n",
            "          -0.05428035,  0.06367354],\n",
            "         [-0.07487888,  0.11612087,  0.00325727, ...,  0.06293631,\n",
            "          -0.02147458,  0.03242005],\n",
            "         [ 0.11080751,  0.0933142 , -0.154461  , ..., -0.05584685,\n",
            "           0.0547305 , -0.04519345]]],\n",
            "\n",
            "\n",
            "       [[[-0.08251072,  0.0893119 ,  0.11391827, ...,  0.13373053,\n",
            "          -0.13459674,  0.11816329],\n",
            "         [-0.03032708, -0.14731626, -0.09024468, ...,  0.01314998,\n",
            "           0.14568648,  0.05143127],\n",
            "         [-0.02023397,  0.11344334,  0.05615669, ..., -0.06437299,\n",
            "           0.01198924, -0.02963026],\n",
            "         ...,\n",
            "         [ 0.05099109,  0.12395263,  0.03060211, ..., -0.04497679,\n",
            "          -0.12949698, -0.08149458],\n",
            "         [-0.11161074,  0.01024681,  0.13918296, ..., -0.07804853,\n",
            "           0.12537631, -0.01805678],\n",
            "         [-0.10457367, -0.13648634, -0.08260979, ...,  0.11022848,\n",
            "           0.13034123,  0.04883154]],\n",
            "\n",
            "        [[-0.05272782, -0.0741674 , -0.02222374, ..., -0.07778816,\n",
            "           0.080385  ,  0.10739315],\n",
            "         [-0.00714447, -0.04845881,  0.14494205, ..., -0.06298536,\n",
            "          -0.11925132,  0.04931685],\n",
            "         [-0.1328939 ,  0.00077333,  0.1561282 , ..., -0.12359646,\n",
            "          -0.13274367,  0.07821926],\n",
            "         ...,\n",
            "         [-0.03300253,  0.13040128, -0.01446916, ...,  0.02200773,\n",
            "          -0.1144978 ,  0.0514527 ],\n",
            "         [ 0.0464575 , -0.01260304,  0.04655975, ..., -0.05035787,\n",
            "           0.01984505,  0.03487544],\n",
            "         [-0.14334996, -0.04729482,  0.09288919, ...,  0.05136625,\n",
            "           0.14852336, -0.10411225]],\n",
            "\n",
            "        [[-0.04498027, -0.07082683, -0.10767898, ..., -0.11240158,\n",
            "           0.12216029, -0.01802434],\n",
            "         [ 0.06769349,  0.02017656, -0.05462902, ...,  0.07377772,\n",
            "           0.15795505,  0.0445227 ],\n",
            "         [-0.01596329, -0.05048695, -0.01808931, ...,  0.00236998,\n",
            "           0.011969  , -0.07945675],\n",
            "         ...,\n",
            "         [ 0.0320532 , -0.07077991,  0.11543974, ..., -0.02293831,\n",
            "          -0.04837699, -0.13891599],\n",
            "         [ 0.08366074, -0.13346171,  0.13517994, ...,  0.04749957,\n",
            "          -0.00331858,  0.08951232],\n",
            "         [ 0.05195865,  0.07418568,  0.12822184, ...,  0.03678007,\n",
            "          -0.08631168, -0.03757814]]]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "      dtype=float32)]\n",
            "[]\n",
            "[array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "      dtype=float32), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "      dtype=float32)]\n",
            "[array([[[[ 0.07646835, -0.03636853, -0.11840212, ...,  0.1161283 ,\n",
            "           0.08600292,  0.00852507],\n",
            "         [-0.10853119, -0.06175583, -0.12688139, ...,  0.08954863,\n",
            "           0.07569715, -0.09432957],\n",
            "         [-0.04754281,  0.10715595, -0.07728826, ..., -0.10047916,\n",
            "           0.10362139, -0.0391124 ],\n",
            "         ...,\n",
            "         [-0.02653746,  0.08448648, -0.04668851, ...,  0.00519519,\n",
            "          -0.1125536 ,  0.03823271],\n",
            "         [-0.03367295, -0.06861921, -0.12676074, ...,  0.1363135 ,\n",
            "           0.00690465, -0.03320993],\n",
            "         [-0.06037966, -0.03268345, -0.02870651, ..., -0.10404126,\n",
            "           0.04481797, -0.03409086]],\n",
            "\n",
            "        [[ 0.14093098, -0.02182185,  0.01613332, ..., -0.06725213,\n",
            "           0.06002527, -0.13153674],\n",
            "         [-0.00750752,  0.01375692, -0.13981897, ...,  0.10674101,\n",
            "           0.07486214,  0.10253777],\n",
            "         [ 0.13395002,  0.11226171, -0.01595926, ...,  0.1316075 ,\n",
            "          -0.08372706, -0.13842793],\n",
            "         ...,\n",
            "         [-0.06870046, -0.00824694, -0.01802905, ..., -0.06661649,\n",
            "          -0.06914297, -0.06163996],\n",
            "         [-0.12541096,  0.09058435,  0.1199739 , ...,  0.09790666,\n",
            "           0.06683894,  0.05928774],\n",
            "         [ 0.07573077,  0.04543424, -0.04554942, ..., -0.03766387,\n",
            "           0.07023145, -0.03321784]],\n",
            "\n",
            "        [[-0.11254238, -0.00800048, -0.00748502, ..., -0.10965408,\n",
            "          -0.02078252,  0.03569329],\n",
            "         [-0.03323295,  0.10679403, -0.13035092, ...,  0.08814111,\n",
            "          -0.12459889,  0.06162313],\n",
            "         [-0.0165408 ,  0.00072211, -0.04718096, ...,  0.03128134,\n",
            "           0.05813953,  0.11906213],\n",
            "         ...,\n",
            "         [ 0.02590729, -0.02416044, -0.10120658, ..., -0.10202657,\n",
            "           0.01200847, -0.07932615],\n",
            "         [-0.13285947,  0.08964789,  0.07143961, ..., -0.02450144,\n",
            "           0.03397393,  0.11762851],\n",
            "         [-0.0217192 ,  0.07574506, -0.03171769, ..., -0.03666937,\n",
            "          -0.06856146,  0.0676772 ]]],\n",
            "\n",
            "\n",
            "       [[[-0.0627955 , -0.11481094,  0.04229091, ..., -0.09481448,\n",
            "          -0.05310525, -0.10457507],\n",
            "         [ 0.10677665, -0.09560848,  0.09911604, ...,  0.06848146,\n",
            "          -0.07770786,  0.05741672],\n",
            "         [-0.1128183 , -0.04204991, -0.07748266, ...,  0.03651528,\n",
            "           0.0050725 ,  0.1024994 ],\n",
            "         ...,\n",
            "         [-0.05797435, -0.04754339, -0.04273118, ...,  0.10717231,\n",
            "          -0.12025397,  0.02156118],\n",
            "         [-0.14287998,  0.04423313,  0.08090033, ...,  0.10554272,\n",
            "           0.07139012, -0.03103246],\n",
            "         [-0.00220697, -0.13280757, -0.12578273, ...,  0.0598985 ,\n",
            "          -0.0424567 , -0.10897601]],\n",
            "\n",
            "        [[-0.05967719, -0.06759106,  0.04243252, ...,  0.12952203,\n",
            "          -0.13258268, -0.14386106],\n",
            "         [-0.03886126, -0.00982353, -0.13222909, ...,  0.05926269,\n",
            "          -0.03617462,  0.14380512],\n",
            "         [-0.00739314, -0.06245716, -0.026088  , ...,  0.02899592,\n",
            "          -0.13403669,  0.08086227],\n",
            "         ...,\n",
            "         [-0.03931124,  0.08834133, -0.02762959, ...,  0.10544258,\n",
            "           0.06258562, -0.00896639],\n",
            "         [ 0.12544775, -0.11591342, -0.00644252, ..., -0.04997152,\n",
            "          -0.01518683,  0.11407375],\n",
            "         [-0.00401948,  0.04639108, -0.01189213, ..., -0.14104725,\n",
            "          -0.05225099, -0.0490654 ]],\n",
            "\n",
            "        [[ 0.08178112,  0.07149984, -0.0947918 , ..., -0.12562504,\n",
            "           0.08991741,  0.0383907 ],\n",
            "         [-0.10376647,  0.12121615,  0.02834438, ..., -0.06163428,\n",
            "           0.07573886, -0.12856056],\n",
            "         [-0.11394838, -0.01364115, -0.05242299, ...,  0.06894296,\n",
            "          -0.0244099 ,  0.08127801],\n",
            "         ...,\n",
            "         [ 0.1363351 , -0.08794655,  0.0122367 , ...,  0.02950735,\n",
            "           0.07840103,  0.04124193],\n",
            "         [-0.13827063, -0.12718655, -0.13250205, ...,  0.01658255,\n",
            "           0.07213128,  0.04954839],\n",
            "         [-0.02834225,  0.13097137, -0.12123594, ...,  0.01497251,\n",
            "           0.03351404, -0.10617867]]],\n",
            "\n",
            "\n",
            "       [[[-0.11360456,  0.07653022,  0.08236369, ..., -0.05963153,\n",
            "           0.13080785,  0.00310259],\n",
            "         [-0.10067463,  0.00447215,  0.06448658, ...,  0.04985155,\n",
            "          -0.02459329, -0.02470089],\n",
            "         [ 0.08345379,  0.00447951, -0.07491457, ...,  0.07029384,\n",
            "           0.13656774,  0.02280568],\n",
            "         ...,\n",
            "         [-0.11654682,  0.112708  , -0.01198758, ..., -0.07964709,\n",
            "          -0.11816122,  0.1321013 ],\n",
            "         [-0.06577039, -0.10948274,  0.10334481, ..., -0.14211814,\n",
            "           0.10515355, -0.10908056],\n",
            "         [ 0.07069024,  0.13139573,  0.04334201, ...,  0.00312045,\n",
            "          -0.06357168, -0.03808198]],\n",
            "\n",
            "        [[ 0.03276844, -0.13099208, -0.06235536, ..., -0.09597618,\n",
            "          -0.12218167, -0.0859874 ],\n",
            "         [-0.09856199, -0.02072082,  0.09476452, ...,  0.13076955,\n",
            "           0.06658782,  0.00265518],\n",
            "         [ 0.04450257, -0.02932731, -0.01189552, ...,  0.13130721,\n",
            "           0.09390154,  0.12853155],\n",
            "         ...,\n",
            "         [ 0.02874057, -0.14073579, -0.03125229, ...,  0.03438474,\n",
            "           0.03974181,  0.06386836],\n",
            "         [-0.11236632, -0.05506878,  0.08469681, ...,  0.07874241,\n",
            "          -0.11788961,  0.03611815],\n",
            "         [ 0.10591766, -0.09909487,  0.0435968 , ...,  0.01963069,\n",
            "           0.00306962,  0.10568234]],\n",
            "\n",
            "        [[ 0.00685503, -0.13832404,  0.13414755, ...,  0.03548154,\n",
            "          -0.14412238, -0.13766348],\n",
            "         [ 0.03987299, -0.13109492, -0.02485166, ...,  0.01781859,\n",
            "          -0.10850635,  0.02201879],\n",
            "         [ 0.09340452,  0.02805638, -0.06877486, ...,  0.00899722,\n",
            "           0.05514796, -0.05839913],\n",
            "         ...,\n",
            "         [-0.00968169,  0.03391036, -0.00151774, ...,  0.00184983,\n",
            "           0.00283293,  0.01975699],\n",
            "         [-0.07996709,  0.05812648, -0.10888912, ..., -0.03964797,\n",
            "          -0.00144203,  0.02790929],\n",
            "         [ 0.05896451,  0.03320009,  0.12006596, ...,  0.13029382,\n",
            "          -0.0913476 ,  0.1130113 ]]]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "      dtype=float32)]\n",
            "[]\n",
            "[array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "      dtype=float32), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "      dtype=float32)]\n",
            "[array([[[[ 9.58227366e-02, -1.38821438e-01, -5.01108617e-02, ...,\n",
            "          -1.38398305e-01, -5.92922196e-02,  7.09524602e-02],\n",
            "         [ 2.21923441e-02,  3.44069302e-03, -6.90103471e-02, ...,\n",
            "           8.07576925e-02, -3.43809575e-02, -1.42855197e-01],\n",
            "         [-1.24609008e-01,  4.90292013e-02,  1.23553127e-02, ...,\n",
            "          -4.65959311e-05,  6.10922128e-02, -1.11849889e-01],\n",
            "         ...,\n",
            "         [ 5.45084924e-02,  1.23203933e-01, -4.56247106e-02, ...,\n",
            "           4.91590649e-02,  8.93902332e-02,  1.01244941e-01],\n",
            "         [ 3.32678109e-02,  1.21109903e-01, -7.22602010e-03, ...,\n",
            "           1.60990506e-02,  3.05529535e-02,  9.10333097e-02],\n",
            "         [ 3.16209495e-02, -6.74929470e-02, -1.90585405e-02, ...,\n",
            "          -7.17847720e-02, -8.75206888e-02, -7.46571720e-02]],\n",
            "\n",
            "        [[-1.20969117e-02,  2.46427059e-02, -1.27406806e-01, ...,\n",
            "           6.40335679e-03, -1.37426078e-01, -1.11252904e-01],\n",
            "         [-1.78740174e-02, -2.69282609e-02,  8.83673429e-02, ...,\n",
            "          -1.11346781e-01,  1.31614506e-01, -8.33940208e-02],\n",
            "         [-2.77152434e-02,  1.62162185e-02,  4.74605560e-02, ...,\n",
            "           1.84036344e-02,  5.47415018e-02, -2.12743431e-02],\n",
            "         ...,\n",
            "         [-9.23417136e-02, -1.42045915e-02, -4.87945378e-02, ...,\n",
            "           1.32184654e-01,  5.76001406e-02,  6.65500760e-03],\n",
            "         [ 6.48526251e-02,  1.15220904e-01,  2.06537843e-02, ...,\n",
            "          -1.16014108e-01, -2.57462487e-02,  2.01687217e-04],\n",
            "         [ 4.01607901e-02, -7.00225979e-02, -7.34176934e-02, ...,\n",
            "           7.10403472e-02, -6.65042326e-02, -1.29499748e-01]],\n",
            "\n",
            "        [[ 8.73337984e-02,  3.75001281e-02,  5.98714501e-02, ...,\n",
            "          -1.25435710e-01,  4.30781990e-02,  9.52231288e-02],\n",
            "         [ 7.66183585e-02, -1.34801343e-01, -7.39102066e-02, ...,\n",
            "           1.05717778e-02,  6.56412393e-02,  7.94994980e-02],\n",
            "         [-1.04243264e-01,  1.32423013e-01, -1.22009329e-01, ...,\n",
            "          -1.22796141e-01, -1.72631890e-02,  8.36816430e-02],\n",
            "         ...,\n",
            "         [-6.12267330e-02, -1.94353238e-02, -1.07125491e-02, ...,\n",
            "          -1.04245707e-01, -2.84671262e-02,  1.20587051e-01],\n",
            "         [ 4.87026572e-03, -3.63289639e-02, -1.29290417e-01, ...,\n",
            "          -1.31915420e-01,  3.45158875e-02, -9.32120830e-02],\n",
            "         [ 3.83573920e-02,  1.37089103e-01, -1.20854095e-01, ...,\n",
            "           4.54657525e-02, -2.60769203e-02, -4.24122810e-03]]],\n",
            "\n",
            "\n",
            "       [[[-1.16290651e-01,  9.10164416e-02, -2.92052105e-02, ...,\n",
            "          -2.83007100e-02,  1.33776814e-02, -3.81885469e-03],\n",
            "         [ 1.17397815e-01, -2.44864970e-02,  1.31182671e-01, ...,\n",
            "           4.91163582e-02,  7.14868903e-02, -1.58667564e-02],\n",
            "         [ 1.22012705e-01,  8.47336352e-02, -2.12526992e-02, ...,\n",
            "           1.04424506e-01,  4.36823517e-02, -1.38167739e-01],\n",
            "         ...,\n",
            "         [-5.84167540e-02, -1.04156822e-01,  1.20566159e-01, ...,\n",
            "          -5.30471951e-02,  1.10157818e-01, -1.02677032e-01],\n",
            "         [ 1.17611021e-01, -5.95247447e-02, -1.16710365e-03, ...,\n",
            "          -6.74804226e-02, -1.24142505e-01,  1.28670335e-02],\n",
            "         [-9.51648355e-02, -4.23205644e-02, -1.03338659e-01, ...,\n",
            "           1.39252424e-01, -3.58777046e-02, -7.31360912e-02]],\n",
            "\n",
            "        [[-1.18911184e-01, -3.45197842e-02, -1.10259026e-01, ...,\n",
            "          -3.18410993e-03, -5.36468402e-02,  2.20611989e-02],\n",
            "         [-5.58698997e-02, -8.50391835e-02,  9.17348415e-02, ...,\n",
            "           4.28803265e-02,  2.15923190e-02,  8.92550945e-02],\n",
            "         [-1.42274246e-01, -1.15500778e-01, -1.28547862e-01, ...,\n",
            "          -5.88669404e-02,  1.33749545e-01,  1.08065635e-01],\n",
            "         ...,\n",
            "         [-3.19833905e-02, -8.48319903e-02,  1.41089410e-01, ...,\n",
            "           9.41416025e-02,  8.48934501e-02,  1.03883401e-01],\n",
            "         [-1.08076498e-01, -9.43558216e-02,  1.27121240e-01, ...,\n",
            "          -1.27521738e-01,  9.65823680e-02,  8.71951431e-02],\n",
            "         [-1.09258741e-02, -9.22968686e-02, -6.64517879e-02, ...,\n",
            "          -1.90554112e-02,  9.18993354e-03, -1.39432684e-01]],\n",
            "\n",
            "        [[ 4.25871015e-02, -1.19293958e-01, -1.12687469e-01, ...,\n",
            "          -1.02684982e-01,  1.12161905e-01, -1.37695372e-02],\n",
            "         [ 3.25593948e-02,  9.09741819e-02,  1.13558352e-01, ...,\n",
            "          -2.86039189e-02, -5.98809123e-03, -1.12050623e-01],\n",
            "         [ 9.72448736e-02, -4.07807380e-02,  2.71390378e-02, ...,\n",
            "          -1.86413974e-02, -4.62191552e-02, -5.20538017e-02],\n",
            "         ...,\n",
            "         [ 1.13589346e-01, -2.60905847e-02,  1.00489378e-01, ...,\n",
            "          -1.12242952e-01,  2.16139704e-02,  5.22595197e-02],\n",
            "         [ 1.08890712e-01,  9.96638238e-02, -3.52810249e-02, ...,\n",
            "          -8.59619677e-03,  8.67275745e-02, -1.27596274e-01],\n",
            "         [ 1.34134293e-03,  1.13132626e-01,  1.15638077e-01, ...,\n",
            "          -4.40120548e-02,  1.14120960e-01,  3.72754782e-02]]],\n",
            "\n",
            "\n",
            "       [[[-1.01318359e-02, -4.63506132e-02, -1.32250637e-01, ...,\n",
            "          -2.91361138e-02,  4.92274165e-02, -8.12130049e-02],\n",
            "         [-6.57552481e-03, -4.58551720e-02,  1.10153288e-01, ...,\n",
            "          -3.61971259e-02, -5.82420751e-02, -1.20047599e-01],\n",
            "         [ 1.43180817e-01, -3.20139825e-02, -1.15675457e-01, ...,\n",
            "           1.20746464e-01, -2.85589769e-02,  5.03778458e-03],\n",
            "         ...,\n",
            "         [-8.41087699e-02, -1.24244392e-03,  3.19219232e-02, ...,\n",
            "           5.90052456e-02, -3.46951485e-02, -3.82452011e-02],\n",
            "         [-2.15422139e-02,  1.63541138e-02,  9.86195654e-02, ...,\n",
            "           6.69464469e-04, -3.53429317e-02, -3.44488919e-02],\n",
            "         [ 5.75238764e-02,  6.40511960e-02,  8.81939083e-02, ...,\n",
            "           1.05207846e-01,  7.28915483e-02, -1.45152211e-04]],\n",
            "\n",
            "        [[ 2.72721052e-03,  1.04553565e-01, -4.22571823e-02, ...,\n",
            "          -1.17611311e-01, -5.52025363e-02,  2.82194316e-02],\n",
            "         [ 1.22527778e-01, -1.13562495e-02, -1.21697001e-01, ...,\n",
            "           6.50580227e-03, -8.94108117e-02, -1.08400218e-01],\n",
            "         [ 7.91071504e-02,  2.30378956e-02, -2.00136006e-02, ...,\n",
            "          -1.38865009e-01,  6.76203072e-02, -1.12050138e-01],\n",
            "         ...,\n",
            "         [ 9.22417492e-02,  4.45810109e-02, -1.23403974e-01, ...,\n",
            "          -7.46264011e-02,  9.00234282e-02,  5.79997748e-02],\n",
            "         [-1.73745602e-02, -4.63426933e-02, -9.31985900e-02, ...,\n",
            "           5.35308719e-02,  1.31797940e-01,  8.20641220e-03],\n",
            "         [-5.14151379e-02, -9.69496146e-02, -9.47633684e-02, ...,\n",
            "          -5.65797016e-02,  1.02673173e-01, -8.42244625e-02]],\n",
            "\n",
            "        [[ 2.84988582e-02,  1.16727352e-02,  8.67724270e-02, ...,\n",
            "           1.05015382e-01,  9.41306651e-02,  8.52336586e-02],\n",
            "         [-1.32714510e-02, -1.20914318e-01, -3.22932377e-02, ...,\n",
            "           1.18974477e-01, -5.91704324e-02, -5.44715747e-02],\n",
            "         [ 3.38131785e-02, -4.06626314e-02,  3.53980213e-02, ...,\n",
            "           2.47540921e-02,  7.93750584e-03, -7.36713484e-02],\n",
            "         ...,\n",
            "         [-3.71062085e-02,  7.44208544e-02,  4.08873409e-02, ...,\n",
            "          -4.68696579e-02, -4.09463942e-02,  4.35816199e-02],\n",
            "         [-1.19255103e-01,  1.33786380e-01, -1.65367723e-02, ...,\n",
            "          -7.43054673e-02, -6.88939616e-02, -6.51298910e-02],\n",
            "         [-5.47334179e-02,  1.25988901e-01, -1.28858015e-01, ...,\n",
            "           1.31545991e-01,  3.30409557e-02, -1.19525485e-01]]]],\n",
            "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "      dtype=float32)]\n",
            "[]\n",
            "[array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "      dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "      dtype=float32), array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "      dtype=float32)]\n",
            "[array([[[[ 0.02291869, -0.01808085,  0.10219419, ..., -0.03773286,\n",
            "           0.00960471, -0.10570364],\n",
            "         [ 0.12392858,  0.05532047, -0.05305442, ..., -0.08229498,\n",
            "           0.10791501,  0.12411582],\n",
            "         [-0.12750037,  0.13352796,  0.04526567, ..., -0.13981077,\n",
            "          -0.06690662,  0.11963859],\n",
            "         ...,\n",
            "         [ 0.04158737, -0.13935605, -0.07555159, ..., -0.0382589 ,\n",
            "          -0.0288849 , -0.10376341],\n",
            "         [ 0.08482614, -0.08513847, -0.11048828, ..., -0.0027235 ,\n",
            "           0.13567632, -0.13908662],\n",
            "         [-0.03653062, -0.03369436,  0.07885569, ..., -0.00957417,\n",
            "          -0.11949338, -0.0476756 ]],\n",
            "\n",
            "        [[ 0.0708759 , -0.09622292, -0.01050033, ...,  0.05001733,\n",
            "          -0.02094167, -0.05872293],\n",
            "         [-0.08031786, -0.00790799, -0.04801963, ...,  0.11047268,\n",
            "          -0.0024592 ,  0.02388386],\n",
            "         [-0.03933884,  0.08742037, -0.13012713, ...,  0.13452703,\n",
            "           0.11658388, -0.07882239],\n",
            "         ...,\n",
            "         [ 0.10923186, -0.12085341, -0.02310875, ..., -0.08870663,\n",
            "           0.1218372 , -0.13688019],\n",
            "         [ 0.08242375, -0.04658069,  0.05136713, ..., -0.02639981,\n",
            "           0.04616874,  0.11484677],\n",
            "         [ 0.0106917 ,  0.10364582,  0.0764858 , ...,  0.06974798,\n",
            "          -0.05449618,  0.04931234]],\n",
            "\n",
            "        [[-0.04471019, -0.07484255, -0.12919785, ..., -0.00947446,\n",
            "           0.00683311,  0.13667765],\n",
            "         [ 0.13735497,  0.1196048 ,  0.06295855, ..., -0.08018589,\n",
            "           0.06260113, -0.07980756],\n",
            "         [ 0.00678375, -0.0774944 , -0.0279422 , ...,  0.00699566,\n",
            "          -0.08630565,  0.0307385 ],\n",
            "         ...,\n",
            "         [ 0.01255295, -0.04544879,  0.03492035, ...,  0.06363966,\n",
            "          -0.04287221,  0.10533656],\n",
            "         [ 0.03350429, -0.07335351,  0.10190102, ..., -0.04798832,\n",
            "          -0.02887244, -0.12883717],\n",
            "         [ 0.02353543,  0.00637101,  0.10919037, ..., -0.10789359,\n",
            "           0.0043762 , -0.05789964]]],\n",
            "\n",
            "\n",
            "       [[[ 0.05101901,  0.069717  ,  0.10850269, ...,  0.01353337,\n",
            "           0.04082829,  0.06792827],\n",
            "         [-0.01647893,  0.00142214,  0.00227214, ..., -0.04411124,\n",
            "           0.04910316, -0.07873467],\n",
            "         [-0.06554701, -0.07348786, -0.08452867, ...,  0.00036773,\n",
            "          -0.09853518, -0.00025503],\n",
            "         ...,\n",
            "         [-0.05171604,  0.09892969,  0.13792828, ..., -0.03383473,\n",
            "           0.06535186,  0.02026606],\n",
            "         [-0.07636635, -0.10597364, -0.11231932, ...,  0.11298144,\n",
            "          -0.08349781,  0.05112332],\n",
            "         [-0.04445798, -0.10672841, -0.11941953, ..., -0.12042511,\n",
            "          -0.00561555, -0.06377327]],\n",
            "\n",
            "        [[-0.09126191, -0.1394141 , -0.01425466, ...,  0.10803387,\n",
            "          -0.08530837,  0.05137287],\n",
            "         [-0.03220876,  0.13356301, -0.05300976, ...,  0.07104665,\n",
            "           0.07525051,  0.09563501],\n",
            "         [-0.11442555, -0.08003847, -0.05011936, ..., -0.07168652,\n",
            "           0.11388671,  0.10686207],\n",
            "         ...,\n",
            "         [ 0.03083393,  0.02089158,  0.14147106, ..., -0.05452715,\n",
            "           0.13557711,  0.02204753],\n",
            "         [ 0.08903512, -0.05719476, -0.03477378, ...,  0.04015855,\n",
            "           0.00783689, -0.12421034],\n",
            "         [-0.03059411, -0.08383419,  0.01590252, ..., -0.00068681,\n",
            "           0.0115471 , -0.12744813]],\n",
            "\n",
            "        [[-0.0997007 ,  0.01488434, -0.11523187, ...,  0.11511019,\n",
            "           0.09767438, -0.13215518],\n",
            "         [-0.08271471,  0.0085281 , -0.07371209, ..., -0.01102647,\n",
            "          -0.08544973, -0.10817482],\n",
            "         [-0.03933189,  0.05120787, -0.08684637, ...,  0.0741341 ,\n",
            "           0.07934752,  0.00586799],\n",
            "         ...,\n",
            "         [ 0.00805458,  0.08180057,  0.08423297, ..., -0.12140126,\n",
            "          -0.05394702,  0.06646298],\n",
            "         [ 0.02806306,  0.02827558, -0.1368202 , ...,  0.0985603 ,\n",
            "          -0.04816912, -0.03474663],\n",
            "         [-0.14141864,  0.00596149,  0.06813234, ..., -0.00046095,\n",
            "          -0.10618318,  0.004375  ]]],\n",
            "\n",
            "\n",
            "       [[[ 0.09814873,  0.0120551 ,  0.0104081 , ..., -0.00474532,\n",
            "          -0.10444781, -0.05855837],\n",
            "         [-0.1387673 , -0.13462043, -0.08163528, ...,  0.00436935,\n",
            "           0.08459808,  0.11370277],\n",
            "         [-0.0175568 , -0.14353357,  0.1381725 , ...,  0.01226002,\n",
            "          -0.08754509, -0.09019938],\n",
            "         ...,\n",
            "         [ 0.07244009, -0.12402292, -0.05389633, ...,  0.08523965,\n",
            "          -0.10728525, -0.06499124],\n",
            "         [ 0.12908196, -0.07473817,  0.04961848, ..., -0.10075697,\n",
            "           0.00237173, -0.10747077],\n",
            "         [ 0.11529011, -0.14319582, -0.00269239, ..., -0.04327752,\n",
            "          -0.12288685,  0.02857839]],\n",
            "\n",
            "        [[ 0.10603669,  0.09939694, -0.02671611, ...,  0.03453389,\n",
            "           0.06336468, -0.01209405],\n",
            "         [-0.10106814, -0.11893634,  0.09134963, ..., -0.10191496,\n",
            "          -0.08784961, -0.06167764],\n",
            "         [ 0.08514094, -0.09575026, -0.10889982, ...,  0.1334525 ,\n",
            "          -0.07203175,  0.13118565],\n",
            "         ...,\n",
            "         [ 0.13229477,  0.03721836,  0.0237851 , ...,  0.03028186,\n",
            "          -0.04796495,  0.10159056],\n",
            "         [ 0.04228348,  0.03679085, -0.03370254, ...,  0.08350551,\n",
            "          -0.09849915, -0.07981575],\n",
            "         [ 0.02879736, -0.09981813,  0.07465421, ..., -0.11991751,\n",
            "          -0.12693496, -0.04281174]],\n",
            "\n",
            "        [[ 0.13221821,  0.04130061, -0.05034494, ...,  0.06865256,\n",
            "           0.05646789, -0.0609219 ],\n",
            "         [ 0.07280295,  0.11480436,  0.07690535, ...,  0.03641644,\n",
            "           0.09085812,  0.01371314],\n",
            "         [ 0.00994395, -0.03394289, -0.03714506, ..., -0.09115764,\n",
            "           0.09753673, -0.02546186],\n",
            "         ...,\n",
            "         [ 0.05385035,  0.13278475, -0.00338605, ...,  0.00915924,\n",
            "          -0.07012814, -0.13384168],\n",
            "         [-0.04065417,  0.14383131, -0.06144711, ..., -0.01349287,\n",
            "          -0.12216969, -0.00066786],\n",
            "         [-0.08975838, -0.01658715,  0.04560076, ...,  0.02554758,\n",
            "          -0.11962663, -0.10361571]]]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "      dtype=float32)]\n",
            "[]\n",
            "[array([[[[-0.04055871, -0.11828544, -0.08885469, ..., -0.03446042,\n",
            "          -0.11688162, -0.08246765],\n",
            "         [ 0.11155683,  0.10613726, -0.05949046, ...,  0.06238854,\n",
            "           0.04509792,  0.08910803],\n",
            "         [ 0.08484617,  0.01348363, -0.11110817, ...,  0.0727005 ,\n",
            "           0.00836718, -0.10498197],\n",
            "         ...,\n",
            "         [-0.10669878,  0.01906876,  0.02200402, ..., -0.08728944,\n",
            "           0.06024709, -0.11153406],\n",
            "         [ 0.01138173, -0.00193804, -0.01535954, ...,  0.01149802,\n",
            "           0.0182952 ,  0.02807303],\n",
            "         [ 0.0496057 , -0.08233476,  0.00551169, ...,  0.0932375 ,\n",
            "          -0.08944276,  0.11860153]],\n",
            "\n",
            "        [[ 0.03793775,  0.09958972,  0.08189109, ..., -0.10002291,\n",
            "           0.03928101, -0.06235595],\n",
            "         [-0.01146583, -0.01684992,  0.10770512, ...,  0.01253027,\n",
            "          -0.09119282,  0.1153128 ],\n",
            "         [-0.0519902 ,  0.02806558, -0.03861105, ..., -0.01566388,\n",
            "          -0.11861853, -0.07071187],\n",
            "         ...,\n",
            "         [ 0.00990452, -0.11165615, -0.05594344, ...,  0.09713794,\n",
            "          -0.08515145,  0.09454536],\n",
            "         [ 0.08080038,  0.07085009, -0.00016272, ...,  0.06194544,\n",
            "          -0.03698337, -0.02900074],\n",
            "         [-0.02577885,  0.06084575, -0.11287533, ...,  0.07453912,\n",
            "          -0.07816265,  0.08722049]],\n",
            "\n",
            "        [[ 0.03697765,  0.01293081, -0.08499637, ..., -0.07679141,\n",
            "           0.03390948,  0.07817061],\n",
            "         [ 0.08825143,  0.04596269,  0.10137668, ..., -0.06084294,\n",
            "          -0.00352485,  0.0459075 ],\n",
            "         [ 0.06201367,  0.09935939, -0.0100588 , ..., -0.04202919,\n",
            "          -0.11123622, -0.03340045],\n",
            "         ...,\n",
            "         [-0.06069528, -0.07858092, -0.03472891, ..., -0.11822002,\n",
            "          -0.00515611, -0.05939474],\n",
            "         [ 0.03731154,  0.00112351, -0.0649818 , ...,  0.0142279 ,\n",
            "           0.00447121, -0.05689988],\n",
            "         [ 0.11593813, -0.01649484, -0.05555475, ...,  0.06021093,\n",
            "           0.04167563,  0.11203428]],\n",
            "\n",
            "        [[ 0.09883079, -0.10694371,  0.02078921, ..., -0.08757785,\n",
            "           0.0095015 ,  0.07239094],\n",
            "         [ 0.11921433, -0.04008904, -0.03411476, ...,  0.03597511,\n",
            "           0.0183958 ,  0.11421238],\n",
            "         [-0.00882047, -0.07340001, -0.09216233, ..., -0.00167816,\n",
            "          -0.00806309, -0.04562674],\n",
            "         ...,\n",
            "         [-0.09336911,  0.06238416, -0.08131178, ..., -0.02565061,\n",
            "          -0.11480139,  0.05354533],\n",
            "         [ 0.02635241, -0.08308039, -0.06502266, ..., -0.0240464 ,\n",
            "           0.02660362, -0.11769027],\n",
            "         [-0.0794944 , -0.00744734,  0.08481401, ...,  0.09192617,\n",
            "          -0.10941026,  0.09413104]]],\n",
            "\n",
            "\n",
            "       [[[-0.04344536, -0.04387262, -0.10655814, ...,  0.06010547,\n",
            "           0.0481241 ,  0.00894436],\n",
            "         [ 0.03941558, -0.06551633,  0.10520181, ...,  0.08899325,\n",
            "           0.01916198, -0.11939967],\n",
            "         [ 0.0570467 , -0.07117774,  0.02797438, ..., -0.11300126,\n",
            "          -0.07539248,  0.04749113],\n",
            "         ...,\n",
            "         [-0.01393412,  0.0517242 ,  0.02317549, ...,  0.01246217,\n",
            "          -0.04248532, -0.09719247],\n",
            "         [-0.04093059, -0.06335548, -0.06748117, ..., -0.0426507 ,\n",
            "           0.10991137,  0.0293581 ],\n",
            "         [-0.01234148, -0.04476432, -0.11758012, ...,  0.08777387,\n",
            "           0.05192836, -0.1177685 ]],\n",
            "\n",
            "        [[ 0.09829876, -0.10177191,  0.02566856, ..., -0.09459426,\n",
            "           0.03858139,  0.11058083],\n",
            "         [ 0.0182329 , -0.0517898 , -0.01743432, ..., -0.01050991,\n",
            "           0.08657388,  0.11407138],\n",
            "         [ 0.08280826, -0.00236931, -0.05215696, ..., -0.02637551,\n",
            "           0.1186396 ,  0.0628456 ],\n",
            "         ...,\n",
            "         [ 0.06908141,  0.10204782,  0.07846147, ..., -0.00047663,\n",
            "          -0.11893939,  0.06675397],\n",
            "         [-0.00884549, -0.00224619,  0.09650828, ...,  0.08432187,\n",
            "           0.03198159,  0.08911958],\n",
            "         [-0.02542068,  0.09381464, -0.03623387, ...,  0.00039869,\n",
            "          -0.05705197,  0.05092399]],\n",
            "\n",
            "        [[-0.07004225, -0.09670874,  0.07234657, ...,  0.0617139 ,\n",
            "           0.07001331, -0.11139817],\n",
            "         [ 0.06213264,  0.06567574, -0.03301156, ..., -0.03042326,\n",
            "           0.08152297,  0.04724775],\n",
            "         [-0.0865176 ,  0.04497537, -0.11404576, ...,  0.07026245,\n",
            "           0.104559  , -0.00921128],\n",
            "         ...,\n",
            "         [-0.07714947,  0.09797232,  0.07643048, ..., -0.11325113,\n",
            "           0.03338942,  0.11039859],\n",
            "         [-0.01213759, -0.08369216, -0.03264602, ...,  0.04409204,\n",
            "           0.08767012,  0.09253144],\n",
            "         [ 0.05199628, -0.11732188,  0.04571241, ..., -0.00704564,\n",
            "           0.02131787, -0.11557297]],\n",
            "\n",
            "        [[-0.03772675, -0.01677296,  0.09546691, ...,  0.00556755,\n",
            "          -0.05906027,  0.07051359],\n",
            "         [-0.01845813, -0.09106448, -0.11289531, ..., -0.04868823,\n",
            "          -0.00514045,  0.00087159],\n",
            "         [ 0.03701364,  0.08056203, -0.00658342, ...,  0.10330123,\n",
            "           0.1059719 ,  0.06069058],\n",
            "         ...,\n",
            "         [-0.04023046,  0.06836504, -0.08550802, ..., -0.01537935,\n",
            "           0.04198535,  0.02003846],\n",
            "         [ 0.09567156,  0.10606372,  0.10786721, ...,  0.00404306,\n",
            "           0.0025617 , -0.06149048],\n",
            "         [-0.10435355, -0.04636316,  0.06823808, ..., -0.08882786,\n",
            "          -0.0584125 , -0.02200571]]],\n",
            "\n",
            "\n",
            "       [[[ 0.07809974, -0.09139242,  0.09980638, ...,  0.0767474 ,\n",
            "           0.09970838, -0.04449725],\n",
            "         [-0.08463769,  0.1111488 ,  0.09250496, ...,  0.11792599,\n",
            "          -0.07172947, -0.09849475],\n",
            "         [-0.0334617 ,  0.08953004, -0.11666813, ..., -0.02358707,\n",
            "           0.06905507, -0.10948648],\n",
            "         ...,\n",
            "         [ 0.04087007, -0.08979999,  0.03429317, ...,  0.07527821,\n",
            "           0.0647026 , -0.11191858],\n",
            "         [-0.07352731, -0.06185037,  0.05901156, ..., -0.08349609,\n",
            "          -0.0587693 , -0.08266023],\n",
            "         [ 0.01457387,  0.10916144,  0.01958084, ...,  0.11964472,\n",
            "          -0.07959722, -0.03751051]],\n",
            "\n",
            "        [[ 0.0829623 ,  0.11468308, -0.06670475, ..., -0.02473283,\n",
            "           0.11027297, -0.09942038],\n",
            "         [-0.11426632, -0.11873556, -0.09147313, ...,  0.05935918,\n",
            "           0.04861218,  0.0729606 ],\n",
            "         [-0.0931531 ,  0.04270665,  0.09061031, ...,  0.04899707,\n",
            "          -0.02655378,  0.05538625],\n",
            "         ...,\n",
            "         [-0.05832277,  0.09626552, -0.08901816, ..., -0.09475782,\n",
            "           0.04069909, -0.11530723],\n",
            "         [-0.05639791, -0.09560856, -0.08691886, ..., -0.116226  ,\n",
            "           0.03633066, -0.05632555],\n",
            "         [-0.01656368,  0.01694602, -0.09500948, ..., -0.10931468,\n",
            "          -0.04100747, -0.03487981]],\n",
            "\n",
            "        [[-0.01114222,  0.09289485, -0.06016966, ..., -0.02995866,\n",
            "          -0.11231827, -0.03464605],\n",
            "         [ 0.09622899,  0.03767867,  0.03074151, ...,  0.06016672,\n",
            "           0.07645009,  0.0153652 ],\n",
            "         [-0.06048798,  0.10768877,  0.01802661, ...,  0.0025052 ,\n",
            "          -0.03224238, -0.0486281 ],\n",
            "         ...,\n",
            "         [-0.06259806,  0.04218386, -0.03127195, ..., -0.04211844,\n",
            "          -0.02719422,  0.03006205],\n",
            "         [ 0.05879945, -0.02352244, -0.06464783, ...,  0.06900997,\n",
            "           0.08395952,  0.03742911],\n",
            "         [-0.09979665,  0.00844654,  0.0563833 , ..., -0.08637904,\n",
            "           0.01702125,  0.0006969 ]],\n",
            "\n",
            "        [[-0.10534532,  0.02887386,  0.09954771, ...,  0.0096707 ,\n",
            "           0.03221403,  0.03413735],\n",
            "         [ 0.02573387,  0.04270828, -0.10409077, ...,  0.05036041,\n",
            "          -0.01241341,  0.04883443],\n",
            "         [-0.09339643, -0.00070443, -0.11941545, ...,  0.00401085,\n",
            "          -0.04719389, -0.04326194],\n",
            "         ...,\n",
            "         [ 0.11128421, -0.05957618,  0.02275985, ...,  0.0540379 ,\n",
            "           0.10440592,  0.05839194],\n",
            "         [ 0.03964964, -0.08239723,  0.11716214, ...,  0.04786938,\n",
            "          -0.11755026,  0.09782666],\n",
            "         [ 0.06703532,  0.04544324, -0.0550033 , ..., -0.08313654,\n",
            "          -0.00827114, -0.08685873]]],\n",
            "\n",
            "\n",
            "       [[[-0.08134548, -0.0601201 ,  0.02083912, ...,  0.01638274,\n",
            "           0.00697881, -0.10257696],\n",
            "         [ 0.09316427,  0.00073814,  0.01193298, ..., -0.07947144,\n",
            "          -0.10415086, -0.00798636],\n",
            "         [-0.03341348, -0.08335821,  0.1107135 , ...,  0.01466349,\n",
            "          -0.09848554,  0.08991896],\n",
            "         ...,\n",
            "         [ 0.09287328,  0.01966181,  0.03471795, ..., -0.07163411,\n",
            "           0.07756284, -0.03290444],\n",
            "         [ 0.06961881, -0.06849587,  0.05883256, ..., -0.11367342,\n",
            "           0.04579528,  0.11184107],\n",
            "         [-0.02425528,  0.08273952, -0.02448288, ...,  0.0337892 ,\n",
            "          -0.09564573,  0.02936541]],\n",
            "\n",
            "        [[ 0.00677554,  0.08047767,  0.07394212, ...,  0.03102562,\n",
            "           0.01028427, -0.06326599],\n",
            "         [ 0.05568515, -0.03451084, -0.0882341 , ...,  0.09200184,\n",
            "           0.01321498, -0.00347266],\n",
            "         [ 0.10855353, -0.03114436,  0.04501577, ..., -0.0547419 ,\n",
            "           0.01556936,  0.08027798],\n",
            "         ...,\n",
            "         [-0.05216572,  0.01510496,  0.11597075, ...,  0.02959889,\n",
            "          -0.05831971, -0.07626682],\n",
            "         [ 0.08571704, -0.10988016, -0.06073374, ...,  0.06750436,\n",
            "           0.10209368, -0.07064851],\n",
            "         [-0.06605569, -0.04804631, -0.02557015, ..., -0.00777902,\n",
            "          -0.00722108, -0.08106527]],\n",
            "\n",
            "        [[-0.10536969,  0.06379175,  0.08663113, ..., -0.03589915,\n",
            "           0.03593609,  0.03177901],\n",
            "         [-0.06931062,  0.06672728,  0.05096668, ...,  0.10451336,\n",
            "           0.05078802, -0.00135864],\n",
            "         [ 0.0564266 ,  0.1112209 , -0.1042501 , ...,  0.04190828,\n",
            "           0.02466069,  0.02985971],\n",
            "         ...,\n",
            "         [ 0.1069385 , -0.10372019, -0.01984832, ..., -0.11974736,\n",
            "          -0.03730615, -0.01015208],\n",
            "         [ 0.07987049, -0.03266457, -0.00407   , ...,  0.10253333,\n",
            "          -0.04265068, -0.07809908],\n",
            "         [ 0.07683879,  0.08153841,  0.01240645, ...,  0.09185515,\n",
            "           0.10338983,  0.11248036]],\n",
            "\n",
            "        [[ 0.00716327,  0.06994247, -0.08884436, ...,  0.09157856,\n",
            "          -0.10317324,  0.00937106],\n",
            "         [ 0.03698148, -0.0634495 , -0.02882559, ..., -0.05475926,\n",
            "           0.03167605,  0.0906096 ],\n",
            "         [ 0.09351923,  0.04893854, -0.09501958, ...,  0.04011032,\n",
            "           0.02780974,  0.10318266],\n",
            "         ...,\n",
            "         [ 0.11288022,  0.06059776,  0.01169769, ..., -0.09893007,\n",
            "          -0.07856643, -0.11410993],\n",
            "         [ 0.04486948,  0.03254913, -0.06181756, ..., -0.01895434,\n",
            "           0.10977466, -0.10551511],\n",
            "         [-0.08408387, -0.04439875, -0.11031733, ...,  0.02206054,\n",
            "           0.11607301,  0.02754694]]]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
            "[]\n",
            "[]\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/40\n",
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.003.\n",
            "60000/60000 [==============================] - 10s 165us/step - loss: 0.2440 - acc: 0.9221 - custom_loss: 156.9962 - val_loss: 6.4446 - val_acc: 0.5777 - val_custom_loss: 163.1967\n",
            "Epoch 2/40\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.\n",
            "60000/60000 [==============================] - 6s 92us/step - loss: 0.0688 - acc: 0.9783 - custom_loss: 156.8209 - val_loss: 5.7786 - val_acc: 0.6206 - val_custom_loss: 162.5308\n",
            "Epoch 3/40\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.\n",
            "60000/60000 [==============================] - 6s 93us/step - loss: 0.0532 - acc: 0.9835 - custom_loss: 156.8054 - val_loss: 3.5468 - val_acc: 0.7593 - val_custom_loss: 160.2989\n",
            "Epoch 4/40\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.\n",
            "60000/60000 [==============================] - 6s 92us/step - loss: 0.0451 - acc: 0.9862 - custom_loss: 156.7972 - val_loss: 3.5171 - val_acc: 0.7615 - val_custom_loss: 160.2693\n",
            "Epoch 5/40\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.\n",
            "60000/60000 [==============================] - 6s 93us/step - loss: 0.0395 - acc: 0.9872 - custom_loss: 156.7916 - val_loss: 2.2461 - val_acc: 0.8472 - val_custom_loss: 158.9982\n",
            "Epoch 6/40\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.\n",
            "60000/60000 [==============================] - 6s 92us/step - loss: 0.0350 - acc: 0.9891 - custom_loss: 156.7871 - val_loss: 5.2044 - val_acc: 0.6579 - val_custom_loss: 161.9566\n",
            "Epoch 7/40\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.\n",
            "60000/60000 [==============================] - 6s 92us/step - loss: 0.0323 - acc: 0.9898 - custom_loss: 156.7845 - val_loss: 4.1387 - val_acc: 0.7236 - val_custom_loss: 160.8908\n",
            "Epoch 8/40\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.\n",
            "60000/60000 [==============================] - 5s 91us/step - loss: 0.0297 - acc: 0.9906 - custom_loss: 156.7819 - val_loss: 4.5184 - val_acc: 0.6995 - val_custom_loss: 161.2706\n",
            "Epoch 9/40\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.\n",
            "60000/60000 [==============================] - 6s 92us/step - loss: 0.0278 - acc: 0.9910 - custom_loss: 156.7800 - val_loss: 3.7813 - val_acc: 0.7467 - val_custom_loss: 160.5334\n",
            "Epoch 10/40\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.\n",
            "60000/60000 [==============================] - 6s 93us/step - loss: 0.0275 - acc: 0.9913 - custom_loss: 156.7796 - val_loss: 3.4334 - val_acc: 0.7668 - val_custom_loss: 160.1856\n",
            "Epoch 11/40\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.\n",
            "60000/60000 [==============================] - 6s 92us/step - loss: 0.0255 - acc: 0.9918 - custom_loss: 156.7777 - val_loss: 3.0910 - val_acc: 0.7917 - val_custom_loss: 159.8431\n",
            "Epoch 12/40\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.\n",
            "60000/60000 [==============================] - 6s 93us/step - loss: 0.0239 - acc: 0.9923 - custom_loss: 156.7761 - val_loss: 4.1234 - val_acc: 0.7251 - val_custom_loss: 160.8756\n",
            "Epoch 13/40\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.\n",
            "60000/60000 [==============================] - 6s 92us/step - loss: 0.0238 - acc: 0.9922 - custom_loss: 156.7760 - val_loss: 3.7649 - val_acc: 0.7441 - val_custom_loss: 160.5170\n",
            "Epoch 14/40\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.\n",
            "60000/60000 [==============================] - 6s 92us/step - loss: 0.0225 - acc: 0.9925 - custom_loss: 156.7747 - val_loss: 3.3692 - val_acc: 0.7746 - val_custom_loss: 160.1214\n",
            "Epoch 15/40\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.\n",
            "60000/60000 [==============================] - 5s 91us/step - loss: 0.0218 - acc: 0.9932 - custom_loss: 156.7740 - val_loss: 4.0626 - val_acc: 0.7277 - val_custom_loss: 160.8148\n",
            "Epoch 16/40\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.\n",
            "60000/60000 [==============================] - 6s 92us/step - loss: 0.0221 - acc: 0.9928 - custom_loss: 156.7742 - val_loss: 3.5223 - val_acc: 0.7622 - val_custom_loss: 160.2744\n",
            "Epoch 17/40\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.\n",
            "60000/60000 [==============================] - 6s 93us/step - loss: 0.0208 - acc: 0.9932 - custom_loss: 156.7730 - val_loss: 3.3210 - val_acc: 0.7722 - val_custom_loss: 160.0731\n",
            "Epoch 18/40\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.\n",
            "60000/60000 [==============================] - 6s 93us/step - loss: 0.0193 - acc: 0.9936 - custom_loss: 156.7714 - val_loss: 3.1643 - val_acc: 0.7815 - val_custom_loss: 159.9164\n",
            "Epoch 19/40\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.\n",
            "60000/60000 [==============================] - 6s 95us/step - loss: 0.0171 - acc: 0.9944 - custom_loss: 156.7693 - val_loss: 3.8122 - val_acc: 0.7419 - val_custom_loss: 160.5644\n",
            "Epoch 20/40\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.\n",
            "60000/60000 [==============================] - 6s 95us/step - loss: 0.0187 - acc: 0.9938 - custom_loss: 156.7709 - val_loss: 4.0034 - val_acc: 0.7313 - val_custom_loss: 160.7556\n",
            "Epoch 21/40\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0004065041.\n",
            "60000/60000 [==============================] - 5s 92us/step - loss: 0.0172 - acc: 0.9944 - custom_loss: 156.7694 - val_loss: 4.0696 - val_acc: 0.7281 - val_custom_loss: 160.8217\n",
            "Epoch 22/40\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.000389661.\n",
            "60000/60000 [==============================] - 6s 92us/step - loss: 0.0166 - acc: 0.9948 - custom_loss: 156.7687 - val_loss: 4.5077 - val_acc: 0.7015 - val_custom_loss: 161.2599\n",
            "Epoch 23/40\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0003741581.\n",
            "60000/60000 [==============================] - 6s 93us/step - loss: 0.0165 - acc: 0.9947 - custom_loss: 156.7686 - val_loss: 4.2221 - val_acc: 0.7202 - val_custom_loss: 160.9743\n",
            "Epoch 24/40\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0003598417.\n",
            "60000/60000 [==============================] - 6s 93us/step - loss: 0.0165 - acc: 0.9945 - custom_loss: 156.7687 - val_loss: 3.5555 - val_acc: 0.7594 - val_custom_loss: 160.3076\n",
            "Epoch 25/40\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0003465804.\n",
            "60000/60000 [==============================] - 5s 92us/step - loss: 0.0150 - acc: 0.9950 - custom_loss: 156.7671 - val_loss: 4.3155 - val_acc: 0.7181 - val_custom_loss: 161.0677\n",
            "Epoch 26/40\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0003342618.\n",
            "60000/60000 [==============================] - 6s 92us/step - loss: 0.0165 - acc: 0.9946 - custom_loss: 156.7686 - val_loss: 4.1634 - val_acc: 0.7261 - val_custom_loss: 160.9156\n",
            "Epoch 27/40\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0003227889.\n",
            "60000/60000 [==============================] - 5s 92us/step - loss: 0.0154 - acc: 0.9951 - custom_loss: 156.7676 - val_loss: 3.6343 - val_acc: 0.7595 - val_custom_loss: 160.3864\n",
            "Epoch 28/40\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0003120774.\n",
            "60000/60000 [==============================] - 6s 92us/step - loss: 0.0150 - acc: 0.9952 - custom_loss: 156.7672 - val_loss: 4.2017 - val_acc: 0.7227 - val_custom_loss: 160.9538\n",
            "Epoch 29/40\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.000302054.\n",
            "60000/60000 [==============================] - 6s 92us/step - loss: 0.0147 - acc: 0.9952 - custom_loss: 156.7669 - val_loss: 4.1248 - val_acc: 0.7275 - val_custom_loss: 160.8770\n",
            "Epoch 30/40\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0002926544.\n",
            "60000/60000 [==============================] - 5s 92us/step - loss: 0.0141 - acc: 0.9955 - custom_loss: 156.7662 - val_loss: 4.0152 - val_acc: 0.7356 - val_custom_loss: 160.7674\n",
            "Epoch 31/40\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.0002838221.\n",
            "60000/60000 [==============================] - 6s 92us/step - loss: 0.0138 - acc: 0.9954 - custom_loss: 156.7660 - val_loss: 4.0230 - val_acc: 0.7322 - val_custom_loss: 160.7752\n",
            "Epoch 32/40\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 0.0002755074.\n",
            "60000/60000 [==============================] - 5s 91us/step - loss: 0.0143 - acc: 0.9955 - custom_loss: 156.7665 - val_loss: 4.2831 - val_acc: 0.7176 - val_custom_loss: 161.0353\n",
            "Epoch 33/40\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.000267666.\n",
            "60000/60000 [==============================] - 6s 92us/step - loss: 0.0141 - acc: 0.9955 - custom_loss: 156.7662 - val_loss: 3.8020 - val_acc: 0.7496 - val_custom_loss: 160.5542\n",
            "Epoch 34/40\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.0002602585.\n",
            "60000/60000 [==============================] - 5s 91us/step - loss: 0.0130 - acc: 0.9956 - custom_loss: 156.7651 - val_loss: 3.7176 - val_acc: 0.7572 - val_custom_loss: 160.4697\n",
            "Epoch 35/40\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.00025325.\n",
            "60000/60000 [==============================] - 6s 92us/step - loss: 0.0143 - acc: 0.9953 - custom_loss: 156.7664 - val_loss: 3.6345 - val_acc: 0.7588 - val_custom_loss: 160.3866\n",
            "Epoch 36/40\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.0002466091.\n",
            "60000/60000 [==============================] - 6s 92us/step - loss: 0.0131 - acc: 0.9959 - custom_loss: 156.7653 - val_loss: 3.9092 - val_acc: 0.7423 - val_custom_loss: 160.6613\n",
            "Epoch 37/40\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.0002403076.\n",
            "60000/60000 [==============================] - 6s 92us/step - loss: 0.0125 - acc: 0.9959 - custom_loss: 156.7647 - val_loss: 4.1222 - val_acc: 0.7286 - val_custom_loss: 160.8744\n",
            "Epoch 38/40\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.0002343201.\n",
            "60000/60000 [==============================] - 6s 92us/step - loss: 0.0125 - acc: 0.9960 - custom_loss: 156.7646 - val_loss: 4.1930 - val_acc: 0.7247 - val_custom_loss: 160.9452\n",
            "Epoch 39/40\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 0.0002286237.\n",
            "60000/60000 [==============================] - 6s 92us/step - loss: 0.0125 - acc: 0.9961 - custom_loss: 156.7646 - val_loss: 3.9905 - val_acc: 0.7357 - val_custom_loss: 160.7426\n",
            "Epoch 40/40\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 0.0002231977.\n",
            "60000/60000 [==============================] - 6s 92us/step - loss: 0.0119 - acc: 0.9962 - custom_loss: 156.7640 - val_loss: 4.1784 - val_acc: 0.7254 - val_custom_loss: 160.9306\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "batch_normalization_1 (Batch (None, 28, 28, 1)         4         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 12)        120       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 26, 26, 12)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 26, 26, 12)        48        \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 24, 24, 20)        2180      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 24, 24, 20)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 24, 24, 10)        210       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 10)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 12, 12, 10)        40        \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 10, 10, 16)        1456      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 10, 10, 16)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 10, 10, 16)        64        \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 8, 8, 16)          2320      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 8, 8, 16)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 8, 8, 16)          64        \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 6, 6, 16)          2320      \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 6, 6, 16)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 6, 6, 16)          64        \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 4, 4, 16)          2320      \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 4, 4, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 1, 1, 10)          2570      \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 13,780\n",
            "Trainable params: 13,638\n",
            "Non-trainable params: 142\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp6SuGrL9M3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from keras.optimizers import Adam\n",
        "# from keras.callbacks import LearningRateScheduler\n",
        "# def scheduler(epoch, lr):\n",
        "#   return round(0.003 * 1/(1 + 0.319 * epoch), 10)\n",
        "\n",
        "# model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.003), metrics=['accuracy'])\n",
        "\n",
        "# model.fit(X_train, Y_train, batch_size=128, epochs=20, verbose=1, validation_data=(X_test, Y_test), callbacks=[LearningRateScheduler(scheduler, verbose=1)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtsH-lLk-eLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0)                               # Evaluate the Model over test data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkX8JMv79q9r",
        "colab_type": "code",
        "outputId": "49a5a2c3-3fa2-49a0-f3cc-84ee4ff4ba25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(score)                                                                    # Accuracy over test data."
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4.178411547088623, 0.7254, 160.93056025390624]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCWoJkwE9suh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(X_test)                                                  # predicting the values with validation accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym7iCFBm9uBs",
        "colab_type": "code",
        "outputId": "4e2622eb-881f-435f-d2ca-31c04eddcedd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        }
      },
      "source": [
        "print(y_pred[:9])                                                               \n",
        "print(y_test[:9])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
            " [0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
            " [1.0475498e-25 1.6321155e-35 4.5395174e-16 3.7464698e-19 9.0166877e-31\n",
            "  4.5100969e-27 6.1507831e-27 0.0000000e+00 1.0000000e+00 4.1075908e-25]\n",
            " [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
            " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00\n",
            "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.3988874e-20]\n",
            " [0.0000000e+00 3.0359957e-30 5.6486189e-09 5.0293702e-33 4.6899813e-18\n",
            "  0.0000000e+00 1.5067583e-34 0.0000000e+00 5.1095122e-01 4.8904881e-01]\n",
            " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00]\n",
            " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
            "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00]\n",
            " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
            "  1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]]\n",
            "[7 2 1 0 4 1 4 9 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAubxDZOE-v9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "bdaab5c3-a3f8-4b54-fa4d-82ea53a72a01"
      },
      "source": [
        "# # Drive Fromalities\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT63m47NFkrr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "eabffa78-a2e2-451b-b7f3-b9b77e975cb9"
      },
      "source": [
        "# import os\n",
        "# print( os.getcwd() )\n",
        "# print( os.listdir() )"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "['.config', 'foo6.png', 'foo60.png', 'foo14.png', 'foo46.png', 'foo69.png', 'foo11.png', 'foo33.png', 'foo5.png', 'foo74.png', 'foo29.png', 'foo67.png', 'myzip.zip', 'foo40.png', 'foo39.png', 'foo31.png', 'foo80.png', 'foo57.png', 'foo17.png', 'foo37.png', 'foo24.png', 'foo70.png', 'foo21.png', 'gdrive', 'foo2.png', 'foo41.png', 'foo79.png', 'foo26.png', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kq_AdfKFGFz4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import files\n",
        "# list_image = ['foo6.png', 'foo60.png', 'foo14.png', 'foo46.png', 'foo69.png', 'foo11.png', 'foo33.png', \n",
        "#               'foo5.png', 'foo74.png', 'foo29.png', 'foo67.png', 'foo40.png', 'foo39.png', 'foo31.png', \n",
        "#               'foo80.png', 'foo57.png', 'foo17.png', 'foo37.png', 'foo24.png', 'foo70.png', 'foo21.png',\n",
        "#               'foo2.png', 'foo41.png', 'foo79.png', 'foo26.png']\n",
        "# len(list_image)\n",
        "# # for i in list_image:\n",
        "#   # print(i)\n",
        "#   # files.download(i)\n",
        "# files.download('foo26.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytqa_J-8AnTg",
        "colab_type": "code",
        "outputId": "29476a5b-d31e-47fe-c561-9245d5930cf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        }
      },
      "source": [
        "# # Misclassified Images \n",
        "# print(y_pred.shape) # (10000, 10)\n",
        "# l =[]\n",
        "# X_test = X_test.reshape(X_test.shape[0], 28, 28)\n",
        "# for i in range(10000):\n",
        "#   if(y_pred[i,y_test[i]] <0.5):\n",
        "#     # print('Value of :',y_test[i],' is ',y_pred[i,y_test[i]])\n",
        "#     l.append(i)\n",
        "\n",
        "# import zipfile\n",
        "# zf = zipfile.ZipFile('myzip.zip', mode='w')\n",
        "# print(l)\n",
        "# # First 25 Misclassified Images\n",
        "# l2 = l[:25]\n",
        "# # print(l2)\n",
        "# from matplotlib import pyplot as plt\n",
        "# # %matplotlib inline\n",
        "# # for i in l2:\n",
        "# #   print(i, y_test[i] , y_pred[i,y_test[i]] )\n",
        "# #   image = plt.imshow(X_test[i])  \n",
        "# #   image.savefig('foo'+str(i)+'.png')\n",
        "# image = plt.imshow(X_test[325])  \n",
        "# image.savefig('foo325.png')\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 10)\n",
            "[2, 5, 6, 11, 14, 17, 21, 24, 26, 29, 31, 33, 37, 39, 40, 41, 46, 57, 60, 67, 69, 70, 74, 79, 80, 83, 89, 94, 95, 96, 97, 107, 110, 111, 115, 124, 128, 135, 137, 139, 143, 145, 154, 163, 166, 168, 171, 175, 176, 177, 178, 180, 184, 189, 190, 191, 196, 202, 203, 204, 217, 220, 223, 224, 226, 228, 229, 233, 239, 242, 243, 246, 247, 250, 251, 254, 255, 257, 262, 263, 265, 267, 272, 276, 279, 282, 286, 287, 288, 290, 297, 300, 302, 307, 314, 324, 325, 329, 330, 332, 337, 338, 342, 345, 348, 350, 354, 357, 358, 360, 368, 372, 376, 377, 378, 385, 388, 393, 398, 399, 405, 409, 410, 411, 413, 415, 416, 419, 420, 425, 427, 430, 435, 438, 439, 445, 449, 455, 462, 465, 468, 473, 476, 480, 489, 495, 504, 506, 508, 510, 519, 522, 523, 527, 529, 532, 533, 534, 537, 542, 543, 550, 551, 552, 554, 557, 565, 571, 579, 580, 584, 591, 605, 610, 615, 617, 618, 619, 625, 631, 636, 640, 647, 649, 650, 651, 652, 655, 658, 663, 666, 667, 668, 672, 675, 682, 684, 688, 689, 691, 695, 696, 698, 700, 702, 703, 705, 707, 708, 709, 716, 723, 725, 726, 730, 735, 736, 740, 745, 746, 748, 749, 754, 755, 761, 762, 767, 768, 771, 772, 774, 777, 783, 784, 789, 790, 798, 800, 803, 809, 818, 822, 824, 825, 826, 827, 829, 831, 832, 835, 836, 840, 841, 842, 846, 848, 850, 852, 853, 859, 870, 878, 881, 889, 895, 898, 900, 901, 902, 907, 909, 911, 916, 917, 918, 920, 929, 930, 931, 936, 940, 946, 947, 948, 949, 950, 956, 959, 960, 961, 963, 964, 967, 969, 977, 979, 984, 987, 988, 994, 998, 1004, 1006, 1008, 1010, 1011, 1012, 1014, 1017, 1019, 1021, 1023, 1024, 1025, 1026, 1027, 1029, 1030, 1037, 1039, 1040, 1051, 1052, 1054, 1055, 1059, 1062, 1071, 1075, 1077, 1078, 1083, 1090, 1096, 1097, 1101, 1108, 1110, 1112, 1117, 1119, 1121, 1124, 1125, 1126, 1128, 1129, 1133, 1136, 1137, 1139, 1143, 1145, 1149, 1153, 1156, 1157, 1159, 1167, 1173, 1175, 1178, 1179, 1180, 1182, 1186, 1189, 1193, 1194, 1198, 1201, 1202, 1206, 1209, 1211, 1213, 1214, 1216, 1219, 1222, 1226, 1229, 1232, 1236, 1238, 1240, 1241, 1242, 1244, 1247, 1250, 1253, 1254, 1257, 1260, 1263, 1270, 1271, 1274, 1276, 1279, 1280, 1292, 1295, 1301, 1302, 1305, 1314, 1316, 1318, 1319, 1321, 1326, 1329, 1338, 1346, 1347, 1350, 1351, 1353, 1355, 1357, 1358, 1360, 1364, 1366, 1368, 1377, 1386, 1391, 1392, 1394, 1397, 1401, 1403, 1413, 1422, 1424, 1427, 1430, 1433, 1434, 1436, 1437, 1448, 1453, 1465, 1477, 1483, 1484, 1488, 1494, 1497, 1498, 1500, 1502, 1505, 1507, 1509, 1515, 1520, 1522, 1527, 1528, 1530, 1534, 1540, 1541, 1543, 1548, 1549, 1555, 1563, 1564, 1566, 1576, 1581, 1582, 1583, 1595, 1606, 1614, 1621, 1624, 1625, 1626, 1627, 1630, 1633, 1634, 1643, 1646, 1652, 1657, 1659, 1660, 1663, 1665, 1667, 1669, 1671, 1673, 1674, 1676, 1679, 1686, 1687, 1688, 1691, 1694, 1702, 1703, 1704, 1705, 1707, 1709, 1710, 1715, 1716, 1717, 1718, 1721, 1728, 1729, 1734, 1736, 1750, 1753, 1754, 1760, 1766, 1772, 1773, 1778, 1780, 1782, 1783, 1785, 1791, 1792, 1800, 1805, 1807, 1809, 1814, 1815, 1816, 1818, 1819, 1820, 1822, 1827, 1829, 1830, 1834, 1835, 1836, 1838, 1841, 1843, 1844, 1848, 1850, 1852, 1861, 1862, 1864, 1865, 1867, 1868, 1876, 1878, 1883, 1884, 1885, 1887, 1897, 1899, 1900, 1901, 1903, 1909, 1916, 1922, 1925, 1933, 1934, 1935, 1938, 1939, 1941, 1945, 1949, 1950, 1955, 1966, 1968, 1973, 1975, 1981, 1982, 1986, 1987, 1988, 1989, 1993, 1994, 1996, 2004, 2005, 2010, 2013, 2015, 2016, 2017, 2018, 2027, 2028, 2034, 2036, 2038, 2041, 2043, 2048, 2051, 2053, 2056, 2059, 2070, 2071, 2079, 2084, 2090, 2091, 2094, 2095, 2099, 2107, 2111, 2115, 2116, 2117, 2118, 2120, 2123, 2129, 2130, 2135, 2136, 2137, 2141, 2143, 2145, 2152, 2154, 2164, 2166, 2169, 2171, 2175, 2182, 2183, 2185, 2188, 2193, 2195, 2197, 2201, 2208, 2215, 2218, 2220, 2221, 2226, 2228, 2234, 2235, 2238, 2239, 2240, 2243, 2245, 2251, 2256, 2258, 2261, 2262, 2264, 2266, 2268, 2273, 2276, 2277, 2278, 2283, 2285, 2287, 2289, 2290, 2293, 2302, 2304, 2314, 2315, 2316, 2319, 2324, 2326, 2329, 2332, 2334, 2335, 2343, 2355, 2356, 2357, 2358, 2359, 2365, 2366, 2370, 2371, 2372, 2375, 2377, 2379, 2380, 2381, 2394, 2398, 2404, 2407, 2409, 2411, 2416, 2418, 2421, 2422, 2425, 2430, 2434, 2436, 2440, 2444, 2447, 2451, 2454, 2463, 2464, 2467, 2473, 2480, 2482, 2484, 2489, 2490, 2498, 2504, 2505, 2507, 2510, 2513, 2514, 2519, 2521, 2523, 2524, 2527, 2529, 2539, 2541, 2543, 2547, 2550, 2552, 2553, 2564, 2567, 2568, 2571, 2576, 2577, 2578, 2588, 2590, 2595, 2597, 2599, 2605, 2607, 2609, 2612, 2614, 2620, 2626, 2628, 2631, 2642, 2651, 2654, 2655, 2658, 2659, 2661, 2671, 2674, 2676, 2688, 2693, 2695, 2700, 2704, 2705, 2706, 2708, 2710, 2713, 2715, 2719, 2721, 2722, 2725, 2730, 2734, 2736, 2742, 2746, 2749, 2750, 2753, 2757, 2758, 2761, 2771, 2778, 2786, 2788, 2789, 2803, 2808, 2809, 2815, 2816, 2820, 2822, 2823, 2825, 2827, 2830, 2833, 2834, 2835, 2836, 2837, 2843, 2851, 2860, 2867, 2868, 2877, 2878, 2880, 2882, 2885, 2887, 2888, 2896, 2901, 2902, 2905, 2907, 2912, 2915, 2917, 2927, 2928, 2932, 2935, 2938, 2943, 2946, 2950, 2953, 2960, 2961, 2965, 2968, 2972, 2974, 2979, 2982, 2984, 2990, 2993, 2995, 2997, 2998, 3000, 3002, 3003, 3011, 3019, 3023, 3026, 3027, 3038, 3039, 3040, 3050, 3054, 3055, 3059, 3060, 3061, 3062, 3064, 3065, 3069, 3070, 3073, 3084, 3092, 3097, 3099, 3101, 3103, 3107, 3109, 3111, 3112, 3114, 3122, 3124, 3126, 3132, 3139, 3143, 3148, 3152, 3153, 3154, 3161, 3163, 3166, 3172, 3175, 3182, 3184, 3189, 3192, 3196, 3203, 3211, 3213, 3214, 3216, 3219, 3225, 3227, 3228, 3230, 3231, 3232, 3233, 3237, 3238, 3244, 3253, 3254, 3255, 3259, 3262, 3264, 3266, 3268, 3269, 3272, 3273, 3276, 3277, 3281, 3282, 3288, 3289, 3292, 3308, 3309, 3313, 3314, 3316, 3319, 3320, 3324, 3328, 3329, 3331, 3333, 3349, 3351, 3352, 3353, 3354, 3356, 3360, 3361, 3370, 3373, 3376, 3378, 3379, 3380, 3386, 3388, 3399, 3405, 3410, 3419, 3420, 3421, 3423, 3425, 3429, 3430, 3433, 3434, 3437, 3438, 3441, 3448, 3451, 3452, 3454, 3455, 3457, 3461, 3464, 3471, 3480, 3490, 3492, 3494, 3496, 3497, 3498, 3499, 3509, 3517, 3518, 3520, 3523, 3525, 3530, 3532, 3534, 3535, 3541, 3542, 3546, 3550, 3555, 3559, 3562, 3568, 3571, 3572, 3573, 3574, 3575, 3576, 3582, 3583, 3585, 3586, 3589, 3594, 3598, 3601, 3604, 3605, 3606, 3607, 3609, 3612, 3615, 3616, 3625, 3626, 3637, 3638, 3639, 3641, 3646, 3648, 3649, 3651, 3652, 3656, 3667, 3669, 3673, 3674, 3679, 3681, 3685, 3688, 3689, 3692, 3699, 3702, 3704, 3707, 3718, 3719, 3720, 3721, 3722, 3724, 3726, 3727, 3730, 3733, 3736, 3741, 3745, 3747, 3751, 3761, 3762, 3765, 3767, 3777, 3779, 3780, 3781, 3784, 3786, 3789, 3790, 3794, 3799, 3808, 3809, 3815, 3816, 3818, 3828, 3836, 3841, 3843, 3852, 3853, 3854, 3858, 3866, 3871, 3882, 3894, 3900, 3906, 3919, 3920, 3922, 3930, 3937, 3939, 3941, 3954, 3956, 3971, 3974, 3983, 3987, 3988, 3990, 3996, 3997, 3998, 4001, 4005, 4006, 4007, 4010, 4013, 4014, 4017, 4021, 4022, 4025, 4027, 4032, 4035, 4036, 4039, 4050, 4053, 4057, 4061, 4063, 4064, 4065, 4069, 4071, 4074, 4075, 4077, 4078, 4082, 4083, 4085, 4086, 4089, 4090, 4099, 4102, 4104, 4105, 4107, 4110, 4112, 4119, 4124, 4138, 4139, 4141, 4142, 4144, 4146, 4147, 4151, 4153, 4159, 4165, 4168, 4169, 4170, 4171, 4175, 4178, 4179, 4181, 4185, 4190, 4191, 4194, 4195, 4197, 4199, 4200, 4201, 4212, 4214, 4215, 4216, 4217, 4221, 4223, 4228, 4229, 4232, 4238, 4239, 4249, 4251, 4262, 4264, 4265, 4267, 4268, 4269, 4273, 4291, 4292, 4303, 4304, 4308, 4313, 4318, 4322, 4328, 4331, 4337, 4349, 4357, 4373, 4375, 4380, 4382, 4386, 4390, 4391, 4399, 4400, 4408, 4409, 4414, 4419, 4421, 4428, 4432, 4434, 4450, 4453, 4458, 4459, 4460, 4467, 4471, 4477, 4478, 4483, 4485, 4486, 4487, 4491, 4493, 4497, 4498, 4505, 4507, 4513, 4514, 4516, 4518, 4519, 4522, 4523, 4524, 4525, 4530, 4533, 4536, 4538, 4539, 4540, 4543, 4545, 4547, 4551, 4558, 4561, 4563, 4564, 4567, 4571, 4574, 4575, 4578, 4579, 4580, 4588, 4589, 4593, 4595, 4597, 4598, 4601, 4602, 4603, 4604, 4606, 4612, 4619, 4620, 4623, 4639, 4640, 4643, 4646, 4650, 4651, 4652, 4653, 4654, 4658, 4660, 4667, 4669, 4670, 4674, 4676, 4681, 4685, 4687, 4690, 4691, 4693, 4695, 4708, 4717, 4719, 4721, 4724, 4726, 4729, 4730, 4731, 4732, 4733, 4736, 4738, 4740, 4741, 4742, 4743, 4744, 4746, 4747, 4750, 4751, 4754, 4759, 4760, 4764, 4767, 4773, 4774, 4777, 4778, 4783, 4784, 4793, 4794, 4797, 4798, 4806, 4807, 4814, 4815, 4820, 4823, 4827, 4829, 4837, 4838, 4839, 4842, 4845, 4853, 4857, 4858, 4859, 4860, 4864, 4866, 4868, 4869, 4871, 4872, 4874, 4877, 4880, 4881, 4886, 4893, 4896, 4900, 4903, 4909, 4911, 4917, 4921, 4924, 4927, 4929, 4930, 4931, 4932, 4938, 4949, 4951, 4952, 4953, 4955, 4956, 4957, 4966, 4970, 4972, 4976, 4977, 4978, 4984, 4987, 4993, 4994, 5005, 5013, 5014, 5023, 5025, 5036, 5040, 5044, 5048, 5052, 5058, 5064, 5071, 5072, 5077, 5081, 5090, 5092, 5093, 5094, 5097, 5107, 5108, 5110, 5112, 5113, 5114, 5121, 5122, 5128, 5132, 5148, 5151, 5154, 5156, 5159, 5162, 5165, 5166, 5171, 5172, 5175, 5177, 5179, 5180, 5186, 5190, 5193, 5195, 5199, 5200, 5201, 5203, 5205, 5208, 5211, 5218, 5221, 5227, 5228, 5232, 5235, 5236, 5238, 5239, 5240, 5246, 5252, 5254, 5258, 5261, 5271, 5278, 5287, 5291, 5310, 5313, 5314, 5316, 5318, 5319, 5323, 5324, 5327, 5331, 5348, 5350, 5355, 5360, 5361, 5363, 5365, 5370, 5371, 5372, 5379, 5380, 5384, 5387, 5399, 5406, 5409, 5412, 5416, 5419, 5422, 5425, 5426, 5431, 5433, 5434, 5437, 5439, 5442, 5444, 5450, 5453, 5457, 5460, 5461, 5465, 5466, 5477, 5482, 5490, 5493, 5495, 5496, 5499, 5500, 5505, 5506, 5512, 5514, 5519, 5520, 5524, 5525, 5532, 5533, 5534, 5535, 5540, 5544, 5549, 5550, 5551, 5552, 5553, 5556, 5560, 5564, 5566, 5577, 5588, 5590, 5593, 5594, 5597, 5600, 5604, 5607, 5610, 5613, 5614, 5617, 5620, 5630, 5631, 5634, 5637, 5640, 5642, 5646, 5647, 5648, 5649, 5650, 5651, 5654, 5655, 5657, 5660, 5661, 5663, 5664, 5666, 5670, 5676, 5677, 5678, 5679, 5680, 5688, 5689, 5691, 5696, 5698, 5699, 5709, 5710, 5714, 5720, 5722, 5728, 5732, 5736, 5738, 5741, 5745, 5746, 5751, 5759, 5762, 5773, 5776, 5782, 5783, 5786, 5794, 5795, 5806, 5808, 5809, 5811, 5827, 5831, 5835, 5839, 5845, 5848, 5854, 5858, 5859, 5861, 5866, 5872, 5873, 5880, 5887, 5888, 5889, 5890, 5893, 5896, 5902, 5903, 5906, 5914, 5915, 5916, 5917, 5923, 5933, 5936, 5937, 5939, 5940, 5943, 5949, 5953, 5956, 5959, 5960, 5962, 5969, 5970, 5975, 5976, 5978, 5979, 5981, 5984, 5985, 5986, 5988, 5989, 5991, 5992, 5996, 5998, 5999, 6001, 6004, 6005, 6007, 6008, 6012, 6019, 6024, 6025, 6037, 6038, 6043, 6044, 6054, 6060, 6063, 6069, 6070, 6073, 6079, 6083, 6089, 6090, 6092, 6098, 6101, 6105, 6108, 6110, 6115, 6119, 6121, 6125, 6135, 6141, 6143, 6147, 6149, 6158, 6159, 6161, 6162, 6164, 6170, 6175, 6182, 6188, 6192, 6198, 6202, 6208, 6212, 6219, 6222, 6223, 6224, 6231, 6232, 6233, 6239, 6242, 6243, 6250, 6253, 6254, 6262, 6263, 6267, 6269, 6276, 6278, 6292, 6296, 6306, 6308, 6310, 6313, 6320, 6329, 6335, 6338, 6339, 6344, 6346, 6353, 6356, 6360, 6363, 6364, 6366, 6369, 6372, 6374, 6375, 6377, 6380, 6396, 6397, 6407, 6408, 6410, 6417, 6419, 6426, 6428, 6430, 6434, 6438, 6441, 6445, 6449, 6452, 6456, 6458, 6460, 6468, 6471, 6472, 6477, 6482, 6484, 6490, 6502, 6503, 6506, 6515, 6527, 6533, 6538, 6539, 6540, 6543, 6545, 6546, 6552, 6555, 6556, 6561, 6572, 6576, 6577, 6579, 6586, 6590, 6599, 6601, 6604, 6606, 6609, 6613, 6618, 6619, 6621, 6622, 6623, 6625, 6628, 6629, 6634, 6637, 6640, 6644, 6648, 6651, 6652, 6662, 6666, 6670, 6673, 6674, 6678, 6679, 6680, 6683, 6684, 6694, 6697, 6702, 6708, 6712, 6715, 6720, 6724, 6726, 6729, 6730, 6731, 6734, 6737, 6741, 6749, 6751, 6756, 6759, 6762, 6768, 6769, 6783, 6789, 6791, 6799, 6802, 6809, 6815, 6818, 6819, 6826, 6829, 6834, 6837, 6838, 6839, 6848, 6852, 6853, 6854, 6855, 6856, 6858, 6868, 6869, 6872, 6883, 6891, 6901, 6902, 6911, 6913, 6917, 6928, 6934, 6935, 6938, 6944, 6945, 6948, 6954, 6958, 6960, 6961, 6962, 6963, 6969, 6976, 6983, 6986, 6992, 7000, 7004, 7005, 7014, 7019, 7021, 7023, 7028, 7030, 7032, 7035, 7037, 7041, 7048, 7049, 7053, 7059, 7063, 7069, 7073, 7079, 7087, 7088, 7091, 7094, 7098, 7109, 7121, 7123, 7138, 7141, 7144, 7149, 7165, 7167, 7171, 7173, 7176, 7180, 7183, 7189, 7191, 7197, 7198, 7204, 7210, 7216, 7217, 7219, 7223, 7226, 7228, 7230, 7232, 7238, 7247, 7253, 7260, 7265, 7267, 7268, 7270, 7276, 7280, 7283, 7286, 7290, 7296, 7299, 7300, 7303, 7307, 7319, 7320, 7323, 7325, 7334, 7338, 7340, 7344, 7353, 7354, 7355, 7356, 7362, 7373, 7375, 7390, 7392, 7395, 7399, 7405, 7411, 7413, 7416, 7417, 7418, 7419, 7422, 7423, 7424, 7432, 7434, 7439, 7442, 7445, 7447, 7450, 7453, 7460, 7461, 7477, 7480, 7490, 7493, 7494, 7504, 7507, 7513, 7517, 7523, 7527, 7530, 7533, 7538, 7544, 7556, 7560, 7561, 7565, 7567, 7569, 7573, 7582, 7584, 7589, 7591, 7598, 7600, 7605, 7606, 7608, 7611, 7614, 7618, 7626, 7636, 7638, 7645, 7658, 7661, 7665, 7686, 7691, 7694, 7695, 7696, 7702, 7708, 7711, 7717, 7722, 7725, 7728, 7731, 7734, 7735, 7738, 7744, 7748, 7754, 7757, 7760, 7766, 7783, 7784, 7790, 7796, 7801, 7802, 7806, 7811, 7812, 7822, 7825, 7832, 7839, 7844, 7845, 7847, 7849, 7852, 7853, 7856, 7861, 7865, 7872, 7873, 7877, 7879, 7883, 7885, 7896, 7898, 7899, 7900, 7902, 7903, 7908, 7910, 7911, 7912, 7915, 7916, 7920, 7921, 7923, 7924, 7926, 7927, 7928, 7934, 7940, 7944, 7951, 7954, 7969, 7971, 7973, 7978, 7980, 7985, 7990, 7998, 8000, 8003, 8004, 8005, 8008, 8011, 8016, 8020, 8022, 8030, 8043, 8045, 8048, 8052, 8058, 8061, 8064, 8068, 8074, 8078, 8081, 8086, 8088, 8090, 8095, 8097, 8099, 8100, 8104, 8106, 8107, 8110, 8113, 8124, 8128, 8134, 8138, 8144, 8151, 8159, 8164, 8166, 8175, 8187, 8190, 8202, 8203, 8205, 8218, 8219, 8229, 8231, 8234, 8239, 8244, 8248, 8252, 8257, 8261, 8265, 8268, 8276, 8279, 8282, 8284, 8285, 8286, 8287, 8289, 8294, 8303, 8306, 8311, 8313, 8316, 8321, 8325, 8326, 8329, 8335, 8338, 8340, 8344, 8347, 8350, 8352, 8356, 8357, 8360, 8364, 8367, 8375, 8376, 8382, 8387, 8390, 8391, 8392, 8396, 8398, 8401, 8402, 8403, 8406, 8416, 8417, 8418, 8420, 8427, 8428, 8432, 8439, 8442, 8443, 8449, 8459, 8465, 8468, 8469, 8472, 8475, 8480, 8488, 8490, 8491, 8493, 8497, 8509, 8512, 8519, 8520, 8526, 8527, 8528, 8536, 8538, 8540, 8541, 8543, 8545, 8549, 8555, 8558, 8559, 8566, 8568, 8573, 8575, 8585, 8586, 8600, 8604, 8608, 8617, 8618, 8620, 8631, 8633, 8635, 8636, 8638, 8650, 8651, 8654, 8658, 8659, 8664, 8666, 8667, 8672, 8678, 8682, 8688, 8692, 8698, 8708, 8715, 8718, 8720, 8722, 8724, 8727, 8728, 8729, 8733, 8735, 8736, 8738, 8740, 8742, 8745, 8749, 8756, 8758, 8767, 8768, 8776, 8786, 8799, 8805, 8806, 8809, 8819, 8825, 8826, 8833, 8837, 8850, 8862, 8873, 8874, 8875, 8883, 8885, 8887, 8892, 8904, 8905, 8914, 8918, 8922, 8926, 8930, 8931, 8938, 8956, 8959, 8971, 8974, 8976, 8979, 8980, 8984, 8986, 8988, 8995, 9000, 9002, 9003, 9008, 9009, 9015, 9019, 9024, 9025, 9030, 9031, 9034, 9036, 9044, 9045, 9051, 9061, 9064, 9067, 9068, 9070, 9071, 9074, 9077, 9078, 9081, 9084, 9087, 9090, 9096, 9099, 9102, 9104, 9108, 9110, 9112, 9113, 9120, 9121, 9123, 9124, 9128, 9136, 9140, 9141, 9143, 9147, 9151, 9155, 9164, 9166, 9169, 9171, 9172, 9180, 9186, 9190, 9193, 9196, 9204, 9212, 9222, 9242, 9248, 9249, 9253, 9257, 9262, 9265, 9274, 9279, 9280, 9282, 9291, 9294, 9295, 9300, 9301, 9302, 9313, 9316, 9323, 9324, 9327, 9328, 9335, 9336, 9345, 9348, 9356, 9368, 9371, 9374, 9378, 9384, 9388, 9392, 9393, 9395, 9399, 9401, 9406, 9407, 9411, 9413, 9415, 9419, 9434, 9435, 9438, 9443, 9444, 9447, 9448, 9451, 9458, 9464, 9469, 9485, 9486, 9489, 9492, 9495, 9496, 9499, 9505, 9509, 9518, 9520, 9522, 9534, 9540, 9543, 9546, 9549, 9556, 9560, 9567, 9570, 9573, 9576, 9591, 9593, 9594, 9596, 9599, 9602, 9612, 9615, 9618, 9621, 9622, 9628, 9637, 9640, 9641, 9643, 9644, 9645, 9650, 9652, 9653, 9657, 9660, 9661, 9673, 9674, 9679, 9682, 9684, 9688, 9689, 9690, 9692, 9693, 9698, 9699, 9705, 9711, 9712, 9715, 9721, 9725, 9729, 9731, 9734, 9736, 9737, 9738, 9743, 9745, 9746, 9756, 9760, 9765, 9771, 9772, 9773, 9774, 9780, 9781, 9783, 9788, 9795, 9796, 9799, 9802, 9810, 9815, 9816, 9835, 9836, 9837, 9838, 9840, 9844, 9845, 9859, 9864, 9868, 9869, 9873, 9878, 9884, 9894, 9895, 9896, 9898, 9899, 9903, 9905, 9906, 9909, 9912, 9915, 9916, 9919, 9920, 9922, 9923, 9929, 9931, 9935, 9946, 9947, 9950, 9955, 9956, 9957, 9958, 9959, 9960, 9963, 9968, 9969, 9972, 9978, 9979, 9984, 9991, 9993, 9994]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-1d64a11414bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#   image.savefig('foo'+str(i)+'.png')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m325\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'foo325.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'AxesImage' object has no attribute 'savefig'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}